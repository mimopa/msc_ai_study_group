{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import string\n",
    "import requests\n",
    "import io\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import pickle\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from zipfile import ZipFile\n",
    "from collections import Counter\n",
    "from tensorflow.python.framework import ops\n",
    "import shutil\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_repository = 'temp'\n",
    "shutil.rmtree('temp')\n",
    "if not os.path.exists(local_repository):\n",
    "    from git import Repo\n",
    "    tf_model_repository = 'https://github.com/tensorflow/models'\n",
    "    Repo.clone_from(tf_model_repository, local_repository)\n",
    "    \n",
    "    sys.path.insert(0, 'temp/tutorials/rnn/translate/')\n",
    "    import seq2seq_model as seq2seq_model\n",
    "    import data_utils as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1    # 学習率\n",
    "lr_decay_rate = 0.99   # 減衰率\n",
    "lr_decay_every = 100   # 減衰させる頻度\n",
    "max_gradient = 5.0     # 最大勾配\n",
    "batch_size = 50        # バッチサイズ\n",
    "num_layers = 3         # RNNの層の数\n",
    "rnn_size = 500         # RNNのモデルのサイズ\n",
    "layer_size = 512       # 層のサイズ\n",
    "generations = 10000    # データを処理する回数\n",
    "vocab_size = 10000     # 語彙のサイズ\n",
    "save_every = 1000      # モデルを保存する頻度\n",
    "eval_every = 500       # モデルを評価する頻度\n",
    "output_every = 50      # 出力の頻度\n",
    "punct = string.punctuation  # 句読点を削除\n",
    "\n",
    "# データのダウンロードと格納\n",
    "data_dir = 'temp'\n",
    "data_file = 'eng_ger.txt'\n",
    "model_path = 'seq2seq_model'\n",
    "full_model_dir = os.path.join(data_dir, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_english = ['hello where is my computer',\n",
    "                'the quick brown fox jumped over the lazy dog',\n",
    "                'is it going to rain tomorrow']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data not found, downloading Eng-Ger sentences from www.manythings.org\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    print('Data not found, downloading Eng-Ger sentences from www.manythings.org')\n",
    "    sentence_url = 'http://www.manythings.org/anki/deu-eng.zip'\n",
    "    r = requests.get(sentence_url)\n",
    "    z = ZipFile(io.BytesIO(r.content))\n",
    "    file = z.read('deu.txt')\n",
    "    # Format Data\n",
    "    eng_ger_data = file.decode()\n",
    "    eng_ger_data = eng_ger_data.encode('ascii',errors='ignore')\n",
    "    eng_ger_data = eng_ger_data.decode().split('\\n')\n",
    "    # Write to file\n",
    "    with open(os.path.join(data_dir, data_file), 'w') as out_conn:\n",
    "        for sentence in eng_ger_data:\n",
    "            out_conn.write(sentence + '\\n')\n",
    "else:\n",
    "    eng_ger_data = []\n",
    "    with open(os.path.join(data_dir, data_file), 'r') as in_conn:\n",
    "        for row in in_conn:\n",
    "            eng_ger_data.append(row[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 句読点を削除\n",
    "eng_ger_data = [''.join(char for char in sent if char not in punct) for sent in eng_ger_data]\n",
    "\n",
    "# 各文をタブで分割\n",
    "eng_ger_data = [x.split('\\t') for x in eng_ger_data if len(x)>=1]\n",
    "[english_sentence, german_sentence] = [list(x) for x in zip(*eng_ger_data)]\n",
    "english_sentence = [x.lower().split() for x in english_sentence]\n",
    "german_sentence = [x.lower().split() for x in german_sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 英語の語彙を処理\n",
    "all_english_words = [word for sentence in english_sentence for word in sentence]\n",
    "all_english_counts = Counter(all_english_words)\n",
    "\n",
    "# -1は0（unknown）の分を引くため\n",
    "eng_word_keys = [x[0] for x in all_english_counts.most_common(vocab_size-1)]\n",
    "eng_vocab2ix = dict(zip(eng_word_keys, range(1,vocab_size)))\n",
    "eng_ix2vocab = {val:key for key, val in eng_vocab2ix.items()}\n",
    "english_processed = []\n",
    "for sent in english_sentence:\n",
    "    temp_sentence = []\n",
    "    for word in sent:\n",
    "        try:\n",
    "            temp_sentence.append(eng_vocab2ix[word])\n",
    "        except:\n",
    "            temp_sentence.append(0)\n",
    "    english_processed.append(temp_sentence)\n",
    "\n",
    "\n",
    "# ドイツ語の語彙を処理\n",
    "all_german_words = [word for sentence in german_sentence for word in sentence]\n",
    "all_german_counts = Counter(all_german_words)\n",
    "ger_word_keys = [x[0] for x in all_german_counts.most_common(vocab_size-1)]\n",
    "ger_vocab2ix = dict(zip(ger_word_keys, range(1,vocab_size)))\n",
    "ger_ix2vocab = {val:key for key, val in ger_vocab2ix.items()}\n",
    "german_processed = []\n",
    "for sent in german_sentence:\n",
    "    temp_sentence = []\n",
    "    for word in sent:\n",
    "        try:\n",
    "            temp_sentence.append(ger_vocab2ix[word])\n",
    "        except:\n",
    "            temp_sentence.append(0)\n",
    "    german_processed.append(temp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テスト用の英文を処理し、語彙に含まれていない単語には'0'を使用\n",
    "test_data = []\n",
    "for sentence in test_english:\n",
    "    temp_sentence = []\n",
    "    for word in sentence.split(' '):\n",
    "        try:\n",
    "            temp_sentence.append(eng_vocab2ix[word])\n",
    "        except:\n",
    "            # この単語が語彙に含まれていない場合は'0'を使用\n",
    "            temp_sentence.append(0)\n",
    "    test_data.append(temp_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シーケンスの長さに基づいてバケットを定義し、\n",
    "# データを対応するバケットに分割する\n",
    "x_maxs = [5, 7, 11, 50]\n",
    "y_maxs = [10, 12, 17, 60]\n",
    "buckets = [x for x in zip(x_maxs, y_maxs)]\n",
    "bucketed_data = [[] for _ in range(len(x_maxs))]\n",
    "for eng, ger in zip(english_processed, german_processed):\n",
    "    for ix, (x_max, y_max) in enumerate(zip(x_maxs, y_maxs)):\n",
    "        if (len(eng) <= x_max) and (len(ger) <= y_max):\n",
    "            bucketed_data[ix].append([eng, ger])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation_model(sess, input_vocab_size, output_vocab_size,\n",
    "                      buckets, rnn_size, num_layers, max_gradient,\n",
    "                      learning_rate, lr_decay_rate, forward_only):\n",
    "    model = seq2seq_model.Seq2SeqModel(\n",
    "          input_vocab_size,\n",
    "          output_vocab_size,\n",
    "          buckets,\n",
    "          rnn_size,\n",
    "          num_layers,\n",
    "          max_gradient,\n",
    "          batch_size,\n",
    "          learning_rate,\n",
    "          lr_decay_rate,\n",
    "          forward_only=forward_only,\n",
    "          dtype=tf.float32)\n",
    "    return(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seq2seq_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d1e9b74fde5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m                                     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                     \u001b[0mlr_decay_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr_decay_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                                     forward_only=False)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# テストモデルで同じ変数を再利用\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-93f4506e40af>\u001b[0m in \u001b[0;36mtranslation_model\u001b[0;34m(sess, input_vocab_size, output_vocab_size, buckets, rnn_size, num_layers, max_gradient, learning_rate, lr_decay_rate, forward_only)\u001b[0m\n\u001b[1;32m      2\u001b[0m                       \u001b[0mbuckets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrnn_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_gradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                       learning_rate, lr_decay_rate, forward_only):\n\u001b[0;32m----> 4\u001b[0;31m     model = seq2seq_model.Seq2SeqModel(\n\u001b[0m\u001b[1;32m      5\u001b[0m           \u001b[0minput_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0moutput_vocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'seq2seq_model' is not defined"
     ]
    }
   ],
   "source": [
    "input_vocab_size = vocab_size\n",
    "output_vocab_size = vocab_size\n",
    "\n",
    "translate_model = translation_model(sess=sess,\n",
    "                                    input_vocab_size=vocab_size,\n",
    "                                    output_vocab_size=vocab_size,\n",
    "                                    buckets=buckets,\n",
    "                                    rnn_size=rnn_size,\n",
    "                                    num_layers=num_layers,\n",
    "                                    max_gradient=max_gradient,\n",
    "                                    learning_rate=learning_rate,\n",
    "                                    lr_decay_rate=lr_decay_rate,\n",
    "                                    forward_only=False)\n",
    "\n",
    "# テストモデルで同じ変数を再利用\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "    test_model = translation_model(sess=sess,\n",
    "                                    input_vocab_size=vocab_size,\n",
    "                                    output_vocab_size=vocab_size,\n",
    "                                    buckets=buckets,\n",
    "                                    rnn_size=rnn_size,\n",
    "                                    num_layers=num_layers,\n",
    "                                    max_gradient=max_gradient,\n",
    "                                    learning_rate=learning_rate,\n",
    "                                    lr_decay_rate=lr_decay_rate,\n",
    "                                    forward_only=True)\n",
    "    test_model.batch_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
