{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import string\n",
    "import requests\n",
    "import collections\n",
    "import io\n",
    "import tarfile\n",
    "import gzip\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 100         # 一度にトレーニングする単語のペア数\n",
    "embedding_size = 100    #  トレーニングする各事業の埋め込みサイズ\n",
    "vocabulary_size = 2000 # トレーニングの対象となる単語の数\n",
    "generations = 100000    #  トレーニングの実行回数\n",
    "print_loss_every = 1000  # 1000回おきに損失値を出力\n",
    "\n",
    "num_sampled = int(batch_size/2) # 不正解サンプルの数\n",
    "window_size = 5         # 考慮の対象となる前後の単語の数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# ストップワードを設定\n",
    "nltk.download('stopwords')\n",
    "stops = stopwords.words('english')\n",
    "\n",
    "# 同義語が見つかることを期待して、テストワードを 5 つ設定\n",
    "print_valid_every = 10000\n",
    "valid_words = ['cliche', 'love', 'hate', 'silly', 'sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_movie_data():\n",
    "    save_folder_name = 'temp'\n",
    "    pos_file = os.path.join(save_folder_name, 'rt-polaritydata', 'rt-polarity.pos')\n",
    "    neg_file = os.path.join(save_folder_name, 'rt-polaritydata', 'rt-polarity.neg')\n",
    "    # データがすでにダウンロードされているかどうかを確認\n",
    "    if not os.path.exists(os.path.join(save_folder_name, 'rt-polaritydata')):\n",
    "        movie_data_url = 'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n",
    "        # tar.gz ファイルを保存\n",
    "        req = requests.get(movie_data_url, stream=True)\n",
    "        with open(os.path.join(save_folder_name,'temp_movie_review_temp.tar.gz'), 'wb') as f:\n",
    "            for chunk in req.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    f.flush()\n",
    "        # tar.gz ファイルをtempフォルダに抽出\n",
    "        tar = tarfile.open(os.path.join(save_folder_name,'temp_movie_review_temp.tar.gz'), \"r:gz\")\n",
    "        tar.extractall(path='temp')\n",
    "        tar.close()\n",
    "    pos_data = []\n",
    "    with open(pos_file, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            pos_data.append(line.encode('ascii',errors='ignore').decode())\n",
    "    f.close()\n",
    "    pos_data = [x.rstrip() for x in pos_data]\n",
    "    neg_data = []\n",
    "    with open(neg_file, 'r', encoding='latin-1') as f:\n",
    "        for line in f:\n",
    "            neg_data.append(line.encode('ascii',errors='ignore').decode())\n",
    "    f.close()\n",
    "    neg_data = [x.rstrip() for x in neg_data]  \n",
    "    texts = pos_data + neg_data\n",
    "    target = [1]*len(pos_data) + [0]*len(neg_data)\n",
    "    \n",
    "    return(texts, target)\n",
    "texts, target = load_movie_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# テキストの正規化\n",
    "def normalize_text(texts, stops):\n",
    "    # 小文字に変換\n",
    "    texts = [x.lower() for x in texts]\n",
    "\n",
    "    # 句読点を削除\n",
    "    texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "\n",
    "    # 数字を削除\n",
    "    texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "\n",
    "    # ストップワードを削除\n",
    "    texts = [' '.join([word for word in x.split() if word not in (stops)]) for x in texts]\n",
    "\n",
    "    # 余分なホワイトスペースを削除\n",
    "    texts = [' '.join(x.split()) for x in texts]\n",
    "    \n",
    "    return(texts)\n",
    "    \n",
    "texts = normalize_text(texts, stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = [target[ix] for ix, x in enumerate(texts) if len(x.split()) > 2]\n",
    "texts = [x for x in texts if len(x.split()) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dictionary(sentences, vocabulary_size):\n",
    "    # 文（文字列のリスト）を単語のリストに変換\n",
    "    split_sentences = [s.split() for s in sentences]\n",
    "    words = [x for sublist in split_sentences for x in sublist]\n",
    "    \n",
    "    # 各単語の [word, word_count] のリストを未知のものから初期化\n",
    "    count = [['RARE', -1]]\n",
    "    \n",
    "    # 最も出現頻度の高い単語を N  個まで追加（Nは語彙のサイズ）\n",
    "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
    "    \n",
    "    # ディクショナリを作成\n",
    "    word_dict = {}\n",
    "    # ディクショナリに含めたい単語を追加し\n",
    "    #  ディクショナリの1つ前の長さを値として設定\n",
    "    for word, word_count in count:\n",
    "        word_dict[word] = len(word_dict)\n",
    "    \n",
    "    return(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def text_to_numbers(sentences, word_dict):\n",
    "    # この関数から返すデータを初期化\n",
    "    data = []\n",
    "    for sentence in sentences:\n",
    "        sentence_data = []\n",
    "        # 単語ごとに、選択されたインデックスか'RARE'単語のインデックスを使用\n",
    "        for word in sentence.split(' '):\n",
    "            if word in word_dict:\n",
    "                word_ix = word_dict[word]\n",
    "            else:\n",
    "                word_ix = 0\n",
    "            sentence_data.append(word_ix)\n",
    "        data.append(sentence_data)\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_dictionary = build_dictionary(texts, vocabulary_size)\n",
    "word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))\n",
    "text_data = text_to_numbers(texts, word_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valid_examples = [word_dictionary[x] for x in valid_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_batch_data(sentences, batch_size, window_size, method='skip_gram'):\n",
    "    # バッチデータにデータを設定\n",
    "    batch_data = []\n",
    "    label_data = []\n",
    "    while len(batch_data) < batch_size:\n",
    "        # 最初に文字をランダムに選択\n",
    "        rand_sentence = np.random.choice(sentences)\n",
    "        # 調査の対象となる連続的なウィンドウを生成\n",
    "        window_sequences = [rand_sentence[max((ix-window_size),0):(ix+window_size+1)] for ix, x in enumerate(rand_sentence)]\n",
    "        # 各ウィンドウのどの要素が目的の単語であるかを指定\n",
    "        label_indices = [ix if ix<window_size else window_size for ix,x in enumerate(window_sequences)]\n",
    "        \n",
    "        # ウィンドウごとに目的の単語を抽出し、タプルを作成\n",
    "        if method=='skip_gram':\n",
    "            batch_and_labels = [(x[y], x[:y] + x[(y+1):]) for x,y in zip(window_sequences, label_indices)]\n",
    "            #  タプル（目的の単語、前後の単語）からなる大きなリストを作成\n",
    "            tuple_data = [(x, y_) for x,y in batch_and_labels for y_ in y]\n",
    "        elif method=='cbow':\n",
    "            batch_and_labels = [(x[:y] + x[(y+1):], x[y]) for x,y in zip(window_sequences, label_indices)]\n",
    "            # タプル（目的の単語、前後の単語）からなる大きなリストを作成\n",
    "            tuple_data = [(x_, y) for x,y in batch_and_labels for x_ in x]\n",
    "        else:\n",
    "            raise ValueError('Method {} not implemented yet.'.format(method))\n",
    "            \n",
    "        # バッチとラベルを抽出\n",
    "        batch, labels = [list(x) for x in zip(*tuple_data)]\n",
    "        batch_data.extend(batch[:batch_size])\n",
    "        label_data.extend(labels[:batch_size])\n",
    "    # バッチとラベルをトリミング\n",
    "    batch_data = batch_data[:batch_size]\n",
    "    label_data = label_data[:batch_size]\n",
    "    \n",
    "    # Numpy配列に変換\n",
    "    batch_data = np.array(batch_data)\n",
    "    label_data = np.transpose(np.array([label_data]))\n",
    "    \n",
    "    return(batch_data, label_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "# プレースホルダを作成\n",
    "x_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "y_target = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "# 単語埋め込みを検索\n",
    "embed = tf.nn.embedding_lookup(embeddings, x_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NCE損失関数のパラメータ\n",
    "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                                               stddev=1.0 / np.sqrt(embedding_size)))\n",
    "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "# 予測値をもとに損失値を取得\n",
    "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
    "                                     biases=nce_biases,\n",
    "                                     labels=y_target,\n",
    "                                     inputs=embed,\n",
    "                                     num_sampled=num_sampled,\n",
    "                                     num_classes=vocabulary_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "normalized_embeddings = embeddings / norm\n",
    "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
    "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 1000 : 4.279643535614014\n",
      "Loss at step 2000 : 4.697796821594238\n",
      "Loss at step 3000 : 4.405325412750244\n",
      "Loss at step 4000 : 3.9387598037719727\n",
      "Loss at step 5000 : 3.9294633865356445\n",
      "Loss at step 6000 : 3.9107372760772705\n",
      "Loss at step 7000 : 3.900785207748413\n",
      "Loss at step 8000 : 3.169133186340332\n",
      "Loss at step 9000 : 4.299883842468262\n",
      "Loss at step 10000 : 3.3558096885681152\n",
      "Nearest to cliche: sad, soap, giant, eight, insights,\n",
      "Nearest to love: capture, ill, strong, RARE, cynical,\n",
      "Nearest to hate: RARE, like, mike, menace, darkly,\n",
      "Nearest to silly: fit, many, escapism, crazy, depressing,\n",
      "Nearest to sad: cliche, imagery, human, rap, parents,\n",
      "Loss at step 11000 : 3.7676165103912354\n",
      "Loss at step 12000 : 3.0968637466430664\n",
      "Loss at step 13000 : 4.227167129516602\n",
      "Loss at step 14000 : 3.942843437194824\n",
      "Loss at step 15000 : 4.367527961730957\n",
      "Loss at step 16000 : 3.495479106903076\n",
      "Loss at step 17000 : 3.448532819747925\n",
      "Loss at step 18000 : 4.329442501068115\n",
      "Loss at step 19000 : 3.9101390838623047\n",
      "Loss at step 20000 : 3.694344997406006\n",
      "Nearest to cliche: sad, soap, giant, eight, festival,\n",
      "Nearest to love: capture, ill, RARE, strong, instead,\n",
      "Nearest to hate: mike, menace, darkly, nothing, relationships,\n",
      "Nearest to silly: fit, many, moral, crazy, format,\n",
      "Nearest to sad: human, imagery, cliche, parents, rap,\n",
      "Loss at step 21000 : 3.155190944671631\n",
      "Loss at step 22000 : 2.7774956226348877\n",
      "Loss at step 23000 : 3.7787015438079834\n",
      "Loss at step 24000 : 3.2330870628356934\n",
      "Loss at step 25000 : 4.282878875732422\n",
      "Loss at step 26000 : 3.4862215518951416\n",
      "Loss at step 27000 : 4.272329807281494\n",
      "Loss at step 28000 : 3.8046114444732666\n",
      "Loss at step 29000 : 4.491693496704102\n",
      "Loss at step 30000 : 4.061794281005859\n",
      "Nearest to cliche: sad, soap, eight, giant, last,\n",
      "Nearest to love: capture, ill, RARE, instead, unfortunately,\n",
      "Nearest to hate: mike, nothing, darkly, provide, relationships,\n",
      "Nearest to silly: many, fit, good, moral, notice,\n",
      "Nearest to sad: human, imagery, cliche, parents, rap,\n",
      "Loss at step 31000 : 3.668363094329834\n",
      "Loss at step 32000 : 4.127982139587402\n",
      "Loss at step 33000 : 4.080105781555176\n",
      "Loss at step 34000 : 4.187194347381592\n",
      "Loss at step 35000 : 3.564171075820923\n",
      "Loss at step 36000 : 3.9704251289367676\n",
      "Loss at step 37000 : 3.6622986793518066\n",
      "Loss at step 38000 : 3.646958827972412\n",
      "Loss at step 39000 : 4.206109523773193\n",
      "Loss at step 40000 : 4.329288959503174\n",
      "Nearest to cliche: sad, giant, soap, eight, last,\n",
      "Nearest to love: capture, RARE, ill, instead, anywhere,\n",
      "Nearest to hate: nothing, mike, provide, menace, relationships,\n",
      "Nearest to silly: fit, many, stuff, notice, crazy,\n",
      "Nearest to sad: human, cliche, imagery, parents, rap,\n",
      "Loss at step 41000 : 3.6547393798828125\n",
      "Loss at step 42000 : 3.770580530166626\n",
      "Loss at step 43000 : 3.7563793659210205\n",
      "Loss at step 44000 : 3.9360992908477783\n",
      "Loss at step 45000 : 3.643258571624756\n",
      "Loss at step 46000 : 4.449671268463135\n",
      "Loss at step 47000 : 4.433084011077881\n",
      "Loss at step 48000 : 3.1852402687072754\n",
      "Loss at step 49000 : 3.975815534591675\n",
      "Loss at step 50000 : 4.0410261154174805\n",
      "Nearest to cliche: giant, sad, last, soap, eight,\n",
      "Nearest to love: capture, ill, instead, one, anywhere,\n",
      "Nearest to hate: provide, mike, nothing, menace, relationships,\n",
      "Nearest to silly: many, fit, stuff, notice, memory,\n",
      "Nearest to sad: human, imagery, cliche, parents, rap,\n",
      "Loss at step 51000 : 4.057224273681641\n",
      "Loss at step 52000 : 4.1153059005737305\n",
      "Loss at step 53000 : 2.5402448177337646\n",
      "Loss at step 54000 : 3.9900074005126953\n",
      "Loss at step 55000 : 3.2377002239227295\n",
      "Loss at step 56000 : 3.742269992828369\n",
      "Loss at step 57000 : 3.564373254776001\n",
      "Loss at step 58000 : 3.6890735626220703\n",
      "Loss at step 59000 : 3.050382614135742\n",
      "Loss at step 60000 : 3.792877197265625\n",
      "Nearest to cliche: giant, last, eight, sad, soap,\n",
      "Nearest to love: capture, story, ill, anywhere, one,\n",
      "Nearest to hate: provide, menace, mike, go, house,\n",
      "Nearest to silly: stuff, memory, fit, many, held,\n",
      "Nearest to sad: human, imagery, cliche, parents, showtime,\n",
      "Loss at step 61000 : 4.047662734985352\n",
      "Loss at step 62000 : 3.792841911315918\n",
      "Loss at step 63000 : 3.847945213317871\n",
      "Loss at step 64000 : 3.4833457469940186\n",
      "Loss at step 65000 : 3.835615873336792\n",
      "Loss at step 66000 : 3.8127520084381104\n",
      "Loss at step 67000 : 4.045709609985352\n",
      "Loss at step 68000 : 3.15519380569458\n",
      "Loss at step 69000 : 3.9421331882476807\n",
      "Loss at step 70000 : 3.210479736328125\n",
      "Nearest to cliche: giant, eight, last, sad, soap,\n",
      "Nearest to love: capture, ill, anywhere, story, chase,\n",
      "Nearest to hate: provide, menace, house, go, mike,\n",
      "Nearest to silly: stuff, held, memory, loud, tiresome,\n",
      "Nearest to sad: human, imagery, cliche, showtime, parents,\n",
      "Loss at step 71000 : 3.403714656829834\n",
      "Loss at step 72000 : 3.2273309230804443\n",
      "Loss at step 73000 : 3.4350292682647705\n",
      "Loss at step 74000 : 3.463454246520996\n",
      "Loss at step 75000 : 3.243612766265869\n",
      "Loss at step 76000 : 3.736257314682007\n",
      "Loss at step 77000 : 4.142062187194824\n",
      "Loss at step 78000 : 4.391976833343506\n",
      "Loss at step 79000 : 3.743957757949829\n",
      "Loss at step 80000 : 3.3279106616973877\n",
      "Nearest to cliche: giant, eight, last, enough, popcorn,\n",
      "Nearest to love: capture, ill, anywhere, see, apparently,\n",
      "Nearest to hate: provide, house, menace, get, go,\n",
      "Nearest to silly: stuff, held, loud, tiresome, memory,\n",
      "Nearest to sad: human, imagery, showtime, RARE, parents,\n",
      "Loss at step 81000 : 3.4499154090881348\n",
      "Loss at step 82000 : 3.5559298992156982\n",
      "Loss at step 83000 : 3.7846181392669678\n",
      "Loss at step 84000 : 3.6788647174835205\n",
      "Loss at step 85000 : 3.2573587894439697\n",
      "Loss at step 86000 : 4.092484951019287\n",
      "Loss at step 87000 : 3.4885363578796387\n",
      "Loss at step 88000 : 3.9627416133880615\n",
      "Loss at step 89000 : 3.6698169708251953\n",
      "Loss at step 90000 : 3.466312885284424\n",
      "Nearest to cliche: giant, eight, last, popcorn, enough,\n",
      "Nearest to love: one, capture, better, see, ill,\n",
      "Nearest to hate: provide, go, house, menace, get,\n",
      "Nearest to silly: stuff, loud, action, held, tiresome,\n",
      "Nearest to sad: human, RARE, asks, showtime, imagery,\n",
      "Loss at step 91000 : 4.632156848907471\n",
      "Loss at step 92000 : 3.518028974533081\n",
      "Loss at step 93000 : 3.509232997894287\n",
      "Loss at step 94000 : 3.4904534816741943\n",
      "Loss at step 95000 : 4.003928184509277\n",
      "Loss at step 96000 : 3.3265576362609863\n",
      "Loss at step 97000 : 3.2957944869995117\n",
      "Loss at step 98000 : 3.685389518737793\n",
      "Loss at step 99000 : 4.077925205230713\n",
      "Loss at step 100000 : 3.682889938354492\n",
      "Nearest to cliche: giant, eight, last, expected, transcends,\n",
      "Nearest to love: capture, story, see, little, ill,\n",
      "Nearest to hate: provide, go, get, menace, house,\n",
      "Nearest to silly: stuff, memory, loud, held, action,\n",
      "Nearest to sad: human, asks, showtime, imagery, tales,\n"
     ]
    }
   ],
   "source": [
    "loss_vec = []\n",
    "loss_x_vec = []\n",
    "for i in range(generations):\n",
    "    batch_inputs, batch_labels = generate_batch_data(text_data, batch_size, window_size)\n",
    "    feed_dict = {x_inputs : batch_inputs, y_target : batch_labels}\n",
    "\n",
    "    # トレーニングステップを実行\n",
    "    sess.run(optimizer, feed_dict=feed_dict)\n",
    "\n",
    "    # 損失値を取得\n",
    "    if (i+1) % print_loss_every == 0:\n",
    "        loss_val = sess.run(loss, feed_dict=feed_dict)\n",
    "        loss_vec.append(loss_val)\n",
    "        loss_x_vec.append(i+1)\n",
    "        print(\"Loss at step {} : {}\".format(i+1, loss_val))\n",
    "      \n",
    "    # 検証：テストワードと最も関連する上位5つの単語を出力\n",
    "    if (i+1) % print_valid_every == 0:\n",
    "        sim = sess.run(similarity)\n",
    "        for j in range(len(valid_words)):\n",
    "            valid_word = word_dictionary_rev[valid_examples[j]]\n",
    "            top_k = 5 # 最も近くにある単語の数\n",
    "            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
    "            log_str = \"Nearest to {}:\".format(valid_word)\n",
    "            for k in range(top_k):\n",
    "                close_word = word_dictionary_rev[nearest[k]]\n",
    "                score = sim[j,nearest[k]]\n",
    "                log_str = \"%s %s,\" % (log_str, close_word)\n",
    "            print(log_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
