{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import urllib\n",
    "from tensorflow.python.framework import ops\n",
    "import IPython\n",
    "ops.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "data_dir = 'temp'\n",
    "output_every = 50\n",
    "generations = 20000\n",
    "eval_every = 500\n",
    "image_height = 32\n",
    "image_width = 32\n",
    "crop_height = 24\n",
    "crop_width = 24\n",
    "num_channels = 3\n",
    "num_targets = 10\n",
    "extract_folder = 'cifar-10-batches-bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習率を幾何級数的に減衰させるためのパラメータ\n",
    "learning_rate = 0.1\n",
    "lr_decay = 0.1\n",
    "num_gens_to_wait = 250."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_vec_length = image_height * image_width * num_channels\n",
    "record_length = 1 + image_vec_length # 1はラベル（0-9）の分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データをロードするためのディレクトリとURL\n",
    "data_dir = 'temp'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "cifar10_url = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
    "\n",
    "# ファイルが存在しない場合はデータをダウンロード\n",
    "data_file = os.path.join(data_dir, 'cifar-10-binary.tar.gz')\n",
    "if os.path.isfile(data_file):\n",
    "    pass\n",
    "else:\n",
    "    # ファイルをダウンロード\n",
    "    def progress(block_num, block_size, total_size):\n",
    "        progress_info = [cifar10_url, float(block_num * block_size) / float(total_size) * 100,0]\n",
    "        print('\\r Downloading {} - {:.2f}%'.format(*progress_info), end=\"\")\n",
    "    filepath, _ = urllib.request.urlretrieve(cifar10_url, data_file, progress)\n",
    "    # ファイルを展開\n",
    "    tarfile.open(filepath, 'r:gz').extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cifar_files(filename_queue, distort_images = True):\n",
    "    reader = tf.FixedLengthRecordReader(record_bytes=record_length)\n",
    "    key, record_string = reader.read(filename_queue)\n",
    "    record_bytes = tf.decode_raw(record_string, tf.uint8)\n",
    "    \n",
    "    # ラベルを抽出\n",
    "    image_label = tf.cast(tf.slice(record_bytes, [0], [1]), tf.int32)\n",
    "    \n",
    "    # 画像を抽出\n",
    "    image_extracted = tf.reshape(tf.slice(record_bytes, [1], [image_vec_length]),\n",
    "                                [num_channels, image_height, image_width])\n",
    "    \n",
    "    # 画像を変形\n",
    "    image_uint8image = tf.transpose(image_extracted, [1, 2, 0])\n",
    "    reshaped_image = tf.cast(image_uint8image, tf.float32)\n",
    "    \n",
    "    # 画像をランダムに切り取る\n",
    "    final_image = tf.image.resize_image_with_crop_or_pad(reshaped_image, crop_width, crop_height)\n",
    "    \n",
    "    if distort_images:\n",
    "        # 画像の左右を反転させ、明るさとコントラストを変更\n",
    "        final_image = tf.image.random_flip_left_right(final_image)\n",
    "        final_image = tf.image.random_brightness(final_image,max_delta=63)\n",
    "        final_image = tf.image.random_contrast(final_image,lower=0.2,upper=1.8)\n",
    "        \n",
    "    # 画像を正規化\n",
    "    final_image = tf.image.per_image_standardization(final_image)\n",
    "    return(final_image, image_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_pipeline(batch_size, train_logical=True):\n",
    "    if train_logical:\n",
    "        files = [os.path.join(data_dir, extract_folder, 'data_batch_{}.bin'.format(i)) for i in range(1,6)]\n",
    "    else:\n",
    "        files = [os.path.join(data_dir, extract_folder, 'test_batch.bin')]\n",
    "    filename_queue = tf.train.string_input_producer(files)\n",
    "    image, label = read_cifar_files(filename_queue)\n",
    "    \n",
    "    min_after_dequeue = 5000\n",
    "    capacity = min_after_dequeue + 3 * batch_size\n",
    "    example_batch, label_batch = tf.train.shuffle_batch([image, label],\n",
    "                                                       batch_size=batch_size,\n",
    "                                                       capacity=capacity,\n",
    "                                                       min_after_dequeue=min_after_dequeue)\n",
    "    \n",
    "    return(example_batch, label_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_cnn_model(input_images, batch_size, train_logical=True):\n",
    "    def truncated_normal_var(name, shape, dtype):\n",
    "        return(tf.get_variable(name=name, shape=shape, dtype=dtype, initializer=tf.truncated_normal_initializer(stddev=0.05)))\n",
    "    def zero_var(name, shape, dtype):\n",
    "        return(tf.get_variable(name=name, shape=shape, dtype=dtype, initializer=tf.constant_initializer(0.0)))\n",
    "    \n",
    "    # 1つ目の畳み込み層\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        # 畳み込みカーネルは3色すべてで 5 x 5 、ここでは64個の特徴量を抽出\n",
    "        conv1_kernel = truncated_normal_var(name='conv_kernel1', shape=[5, 5, 3, 64], dtype=tf.float32)\n",
    "        # 画像全体をストライド1で畳み込む\n",
    "        conv1 = tf.nn.conv2d(input_images, conv1_kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        # バイアス項を初期化した上で追加\n",
    "        conv1_bias = zero_var(name='conv_bias1', shape=[64], dtype=tf.float32)\n",
    "        conv1_add_bias = tf.nn.bias_add(conv1, conv1_bias)\n",
    "        # ReLU活性化関数\n",
    "        relu_conv1 = tf.nn.relu(conv1_add_bias)\n",
    "        \n",
    "    # マックスプーリング\n",
    "    pool1 = tf.nn.max_pool(relu_conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool_layer1')\n",
    "    \n",
    "    # LRN （Local Response Normalization）（以下の論文を参照）\n",
    "    # 論文：http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks\n",
    "    norm1 = tf.nn.lrn(pool1, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75, name='norm1')\n",
    "    \n",
    "    # 2つ目の畳み込み層\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        # 畳み込みカーネルは1つ前の64個の特徴量すべてにわたって 5 x 5\n",
    "        # さらに64個の特徴量を抽出\n",
    "        conv2_kernel = truncated_normal_var(name='conv_kernel2', shape=[5, 5, 64, 64], dtype=tf.float32)\n",
    "        # 1つ前の出力全体をストライド1で畳み込む\n",
    "        conv2 = tf.nn.conv2d(norm1, conv2_kernel, [1, 1, 1, 1], padding='SAME')\n",
    "        # バイアス項を初期化した上で追加\n",
    "        conv2_bias = zero_var(name='conv_bias2', shape=[64], dtype=tf.float32)\n",
    "        conv2_add_bias = tf.nn.bias_add(conv2, conv2_bias)\n",
    "        # ReLU活性化関数\n",
    "        relu_conv2 = tf.nn.relu(conv2_add_bias)\n",
    "        \n",
    "    # マックスプーリング\n",
    "    pool2 = tf.nn.max_pool(relu_conv2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool_layer2')\n",
    "    \n",
    "    # LRN（Local Response Normalization）（前の論文を参照）\n",
    "    norm2 = tf.nn.lrn(pool2, depth_radius=5, bias=2.0, alpha=1e-3, beta=0.75, name='norm2')\n",
    "    \n",
    "    # 全結合祖のために出力を1つの行列にまとめて乗算できるようにする\n",
    "    reshaped_output = tf.reshape(norm2, [batch_size, -1])\n",
    "    reshaped_dim = reshaped_output.get_shape()[1].value\n",
    "    \n",
    "    # 1つ目の全結合層\n",
    "    with tf.variable_scope('full1') as scope:\n",
    "        # 1つ目の全結合層の出力は384層\n",
    "        full_weight1 = truncated_normal_var(name='full_mult1', shape=[reshaped_dim, 384], dtype=tf.float32)\n",
    "        full_bias1 = zero_var(name='full_bias1', shape=[384], dtype=tf.float32)\n",
    "        full_layer1 = tf.nn.relu(tf.add(tf.matmul(reshaped_output, full_weight1), full_bias1))\n",
    "        \n",
    "    # 2つ目の全結合層\n",
    "    with tf.variable_scope('full2') as scope:\n",
    "        # 2つ目の全結合層の出力は192個\n",
    "        full_weight2 = truncated_normal_var(name='full_mult2', shape=[384, 192], dtype=tf.float32)\n",
    "        full_bias2 = zero_var(name='full_bias2', shape=[192], dtype=tf.float32)\n",
    "        full_layer2 = tf.nn.relu(tf.add(tf.matmul(full_layer1, full_weight2), full_bias2))\n",
    "        \n",
    "    # 最後の全結合層 -> 出力は10個のカテゴリ\n",
    "    with tf.variable_scope('full3') as scope:\n",
    "        # 最後の全結合層の出力は10個（num_targets）\n",
    "        full_weight3 = truncated_normal_var(name='full_mult3', shape=[192, num_targets], dtype=tf.float32)\n",
    "        full_bias3 = zero_var(name='full_bias3', shape=[num_targets], dtype=tf.float32)\n",
    "        final_output = tf.add(tf.matmul(full_layer2, full_weight3), full_bias3)\n",
    "        \n",
    "    return(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cifar_loss(logits, targets):\n",
    "    # 余分な次元を削除し、目的値を整数にキャスト\n",
    "    targets = tf.squeeze(tf.cast(targets, tf.int32))\n",
    "    # ロジットと目的値\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=targets)\n",
    "    # バッチサイズ全体の損失値を計算\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy, name='cross_entropy')\n",
    "    return(cross_entropy_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(loss_value, generation_num):\n",
    "    # 学習率を幾何級数的に減衰（引き下げ）\n",
    "    model_learning_rate = tf.train.exponential_decay(learning_rate, generation_num,\n",
    "                                                    num_gens_to_wait, lr_decay, staircase=True)\n",
    "    # 最適化関数を作成\n",
    "    my_optimizer = tf.train.GradientDescentOptimizer(model_learning_rate)\n",
    "    # トレーニングステップを初期化\n",
    "    train_step = my_optimizer.minimize(loss_value)\n",
    "    return(train_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_of_batch(logits, targets):\n",
    "    # 目的値が整数であることを確実にし、余分な次元を削除\n",
    "    targets = tf.squeeze(tf.cast(targets, tf.int32))\n",
    "    # ロジットが最大になるものを探すことで、予測値を取得\n",
    "    batch_predictions = tf.cast(tf.argmax(logits, 1), tf.int32)\n",
    "    # それらがバッチ全体で等しいかどうかを確認\n",
    "    predicted_correctly = tf.equal(batch_predictions, targets)\n",
    "    # バッチ全体で1（True）と0（False）の平均を求める\n",
    "    accuracy = tf.reduce_mean(tf.cast(predicted_correctly, tf.float32))\n",
    "    return(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データパイプラインを初期化\n",
    "images, targets = input_pipeline(batch_size, train_logical=True)\n",
    "# パイプラインからテスト用のバッチ画像と目的値を取得\n",
    "test_images, test_targets = input_pipeline(batch_size, train_logical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを設定\n",
    "with tf.variable_scope('model_definition') as scope:\n",
    "    # トレーニングモデルを設定\n",
    "    model_output = cifar_cnn_model(images, batch_size)\n",
    "    # 変数を再利用するためにスコープを設定\n",
    "    scope.reuse_variables()\n",
    "    # テストモデルの出力を設定\n",
    "    test_output = cifar_cnn_model(test_images, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数を設定\n",
    "loss = cifar_loss(model_output, targets)\n",
    "\n",
    "# 正解関数を設定\n",
    "accuracy = accuracy_of_batch(test_output, test_targets)\n",
    "\n",
    "# トレーニング演算を作成\n",
    "generation_num = tf.Variable(0, trainable=False)\n",
    "train_op = train_step(loss, generation_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Thread(Thread-3, started daemon 139939884824320)>,\n",
       " <Thread(Thread-4, started daemon 139939876431616)>,\n",
       " <Thread(Thread-5, started daemon 139939868038912)>,\n",
       " <Thread(Thread-6, started daemon 139939859646208)>]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 変数を初期化\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "# キューを初期化\n",
    "tf.train.start_queue_runners(sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation 50: Loss = 1.97588\n",
      "Generation 100: Loss = 1.89741\n",
      "Generation 150: Loss = 1.90721\n",
      "Generation 200: Loss = 1.67443\n",
      "Generation 250: Loss = 1.59165\n",
      "Generation 300: Loss = 1.64054\n",
      "Generation 350: Loss = 1.55082\n",
      "Generation 400: Loss = 1.47431\n",
      "Generation 450: Loss = 1.41637\n",
      "Generation 500: Loss = 1.33618\n",
      "Generation 550: Loss = 1.32995\n",
      "Generation 600: Loss = 1.35040\n",
      "Generation 650: Loss = 1.15915\n",
      "Generation 700: Loss = 1.25833\n",
      "Generation 750: Loss = 1.25837\n",
      "Generation 800: Loss = 1.13081\n",
      "Generation 850: Loss = 1.14150\n",
      "Generation 900: Loss = 1.37401\n",
      "Generation 950: Loss = 1.26266\n",
      "Generation 1000: Loss = 1.26377\n",
      "Generation 1050: Loss = 1.12136\n",
      "Generation 1100: Loss = 1.19865\n",
      "Generation 1150: Loss = 1.05593\n",
      "Generation 1200: Loss = 1.04289\n",
      "Generation 1250: Loss = 1.22123\n",
      "Generation 1300: Loss = 1.11560\n",
      "Generation 1350: Loss = 0.95127\n",
      "Generation 1400: Loss = 1.13910\n",
      "Generation 1450: Loss = 1.10516\n",
      "Generation 1500: Loss = 1.16113\n",
      "Generation 1550: Loss = 1.11844\n",
      "Generation 1600: Loss = 0.99914\n",
      "Generation 1650: Loss = 0.82940\n",
      "Generation 1700: Loss = 0.70037\n",
      "Generation 1750: Loss = 0.94636\n",
      "Generation 1800: Loss = 0.88971\n",
      "Generation 1850: Loss = 0.86058\n",
      "Generation 1900: Loss = 0.90970\n",
      "Generation 1950: Loss = 0.75988\n",
      "Generation 2000: Loss = 0.91407\n",
      "Generation 2050: Loss = 0.72408\n",
      "Generation 2100: Loss = 0.91524\n",
      "Generation 2150: Loss = 0.82806\n",
      "Generation 2200: Loss = 0.83612\n",
      "Generation 2250: Loss = 0.78879\n",
      "Generation 2300: Loss = 1.00751\n",
      "Generation 2350: Loss = 0.94795\n",
      "Generation 2400: Loss = 0.91711\n",
      "Generation 2450: Loss = 0.74616\n",
      "Generation 2500: Loss = 0.81422\n",
      "Generation 2550: Loss = 1.00739\n",
      "Generation 2600: Loss = 0.72655\n",
      "Generation 2650: Loss = 0.85535\n",
      "Generation 2700: Loss = 0.77236\n",
      "Generation 2750: Loss = 0.80322\n",
      "Generation 2800: Loss = 0.53864\n",
      "Generation 2850: Loss = 0.87338\n",
      "Generation 2900: Loss = 0.73613\n",
      "Generation 2950: Loss = 0.95736\n",
      "Generation 3000: Loss = 0.68169\n",
      "Generation 3050: Loss = 0.61346\n",
      "Generation 3100: Loss = 0.84287\n",
      "Generation 3150: Loss = 0.70765\n",
      "Generation 3200: Loss = 0.59323\n",
      "Generation 3250: Loss = 0.88812\n",
      "Generation 3300: Loss = 0.72520\n",
      "Generation 3350: Loss = 0.84432\n",
      "Generation 3400: Loss = 0.91857\n",
      "Generation 3450: Loss = 0.72704\n",
      "Generation 3500: Loss = 0.54907\n",
      "Generation 3550: Loss = 0.57777\n",
      "Generation 3600: Loss = 0.71872\n",
      "Generation 3650: Loss = 0.78964\n",
      "Generation 3700: Loss = 0.65986\n",
      "Generation 3750: Loss = 0.52467\n",
      "Generation 3800: Loss = 0.56295\n",
      "Generation 3850: Loss = 0.77809\n",
      "Generation 3900: Loss = 0.60984\n",
      "Generation 3950: Loss = 0.59517\n",
      "Generation 4000: Loss = 0.57369\n",
      "Generation 4050: Loss = 0.45022\n",
      "Generation 4100: Loss = 0.62794\n",
      "Generation 4150: Loss = 0.58839\n",
      "Generation 4200: Loss = 0.80123\n",
      "Generation 4250: Loss = 0.44697\n",
      "Generation 4300: Loss = 0.52334\n",
      "Generation 4350: Loss = 0.52805\n",
      "Generation 4400: Loss = 0.56946\n",
      "Generation 4450: Loss = 0.64515\n",
      "Generation 4500: Loss = 0.63929\n",
      "Generation 4550: Loss = 0.58026\n",
      "Generation 4600: Loss = 0.60967\n",
      "Generation 4650: Loss = 0.71335\n",
      "Generation 4700: Loss = 0.57822\n",
      "Generation 4750: Loss = 0.46819\n",
      "Generation 4800: Loss = 0.45539\n",
      "Generation 4850: Loss = 0.78304\n",
      "Generation 4900: Loss = 0.72443\n",
      "Generation 4950: Loss = 0.48498\n",
      "Generation 5000: Loss = 0.58522\n",
      "Generation 5050: Loss = 0.51498\n",
      "Generation 5100: Loss = 0.48620\n",
      "Generation 5150: Loss = 0.42352\n",
      "Generation 5200: Loss = 0.45731\n",
      "Generation 5250: Loss = 0.55822\n",
      "Generation 5300: Loss = 0.44315\n",
      "Generation 5350: Loss = 0.67899\n",
      "Generation 5400: Loss = 0.70830\n",
      "Generation 5450: Loss = 0.56204\n",
      "Generation 5500: Loss = 0.51028\n",
      "Generation 5550: Loss = 0.40636\n",
      "Generation 5600: Loss = 0.47868\n",
      "Generation 5650: Loss = 0.64205\n",
      "Generation 5700: Loss = 0.43575\n",
      "Generation 5750: Loss = 0.56236\n",
      "Generation 5800: Loss = 0.37249\n",
      "Generation 5850: Loss = 0.43749\n",
      "Generation 5900: Loss = 0.50545\n",
      "Generation 5950: Loss = 0.47112\n",
      "Generation 6000: Loss = 0.54061\n",
      "Generation 6050: Loss = 0.35714\n",
      "Generation 6100: Loss = 0.54488\n",
      "Generation 6150: Loss = 0.44231\n",
      "Generation 6200: Loss = 0.32276\n",
      "Generation 6250: Loss = 0.57894\n",
      "Generation 6300: Loss = 0.38051\n",
      "Generation 6350: Loss = 0.37614\n",
      "Generation 6400: Loss = 0.33103\n",
      "Generation 6450: Loss = 0.52378\n",
      "Generation 6500: Loss = 0.29101\n",
      "Generation 6550: Loss = 0.45315\n",
      "Generation 6600: Loss = 0.33609\n",
      "Generation 6650: Loss = 0.40961\n",
      "Generation 6700: Loss = 0.50896\n",
      "Generation 6750: Loss = 0.39623\n",
      "Generation 6800: Loss = 0.29268\n",
      "Generation 6850: Loss = 0.34273\n",
      "Generation 6900: Loss = 0.44084\n",
      "Generation 6950: Loss = 0.29248\n",
      "Generation 7000: Loss = 0.39982\n",
      "Generation 7050: Loss = 0.31187\n",
      "Generation 7100: Loss = 0.34045\n",
      "Generation 7150: Loss = 0.34088\n",
      "Generation 7200: Loss = 0.34709\n",
      "Generation 7250: Loss = 0.30429\n",
      "Generation 7300: Loss = 0.61735\n",
      "Generation 7350: Loss = 0.45597\n",
      "Generation 7400: Loss = 0.35975\n",
      "Generation 7450: Loss = 0.28617\n",
      "Generation 7500: Loss = 0.32689\n",
      "Generation 7550: Loss = 0.33252\n",
      "Generation 7600: Loss = 0.24191\n",
      "Generation 7650: Loss = 0.28341\n",
      "Generation 7700: Loss = 0.34759\n",
      "Generation 7750: Loss = 0.26192\n",
      "Generation 7800: Loss = 0.39233\n",
      "Generation 7850: Loss = 0.36889\n",
      "Generation 7900: Loss = 0.23546\n",
      "Generation 7950: Loss = 0.33841\n",
      "Generation 8000: Loss = 0.30899\n",
      "Generation 8050: Loss = 0.32994\n",
      "Generation 8100: Loss = 0.32803\n",
      "Generation 8150: Loss = 0.26582\n",
      "Generation 8200: Loss = 0.25213\n",
      "Generation 8250: Loss = 0.39979\n",
      "Generation 8300: Loss = 0.21820\n",
      "Generation 8350: Loss = 0.30483\n",
      "Generation 8400: Loss = 0.19955\n",
      "Generation 8450: Loss = 0.37203\n",
      "Generation 8500: Loss = 0.30807\n",
      "Generation 8550: Loss = 0.23903\n",
      "Generation 8600: Loss = 0.28385\n",
      "Generation 8650: Loss = 0.19585\n",
      "Generation 8700: Loss = 0.15871\n",
      "Generation 8750: Loss = 0.24118\n",
      "Generation 8800: Loss = 0.25994\n",
      "Generation 8850: Loss = 0.27549\n",
      "Generation 8900: Loss = 0.26945\n",
      "Generation 8950: Loss = 0.26755\n",
      "Generation 9000: Loss = 0.26797\n",
      "Generation 9050: Loss = 0.16917\n",
      "Generation 9100: Loss = 0.20153\n",
      "Generation 9150: Loss = 0.23236\n",
      "Generation 9200: Loss = 0.30075\n",
      "Generation 9250: Loss = 0.21031\n",
      "Generation 9300: Loss = 0.26943\n",
      "Generation 9350: Loss = 0.22083\n",
      "Generation 9400: Loss = 0.28172\n",
      "Generation 9450: Loss = 0.30628\n",
      "Generation 9500: Loss = 0.25129\n",
      "Generation 9550: Loss = 0.20667\n",
      "Generation 9600: Loss = 0.20275\n",
      "Generation 9650: Loss = 0.25814\n",
      "Generation 9700: Loss = 0.19082\n",
      "Generation 9750: Loss = 0.18246\n",
      "Generation 9800: Loss = 0.16480\n",
      "Generation 9850: Loss = 0.28290\n",
      "Generation 9900: Loss = 0.20176\n",
      "Generation 9950: Loss = 0.27892\n",
      "Generation 10000: Loss = 0.31257\n",
      "Generation 10050: Loss = 0.22426\n",
      "Generation 10100: Loss = 0.16796\n",
      "Generation 10150: Loss = 0.42432\n",
      "Generation 10200: Loss = 0.09645\n",
      "Generation 10250: Loss = 0.20614\n",
      "Generation 10300: Loss = 0.27981\n",
      "Generation 10350: Loss = 0.22759\n",
      "Generation 10400: Loss = 0.19670\n",
      "Generation 10450: Loss = 0.12086\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "test_accuracy = []\n",
    "for i in range(generations):\n",
    "    _, loss_value = sess.run([train_op, loss])\n",
    "    \n",
    "    if (i+1) % output_every == 0:\n",
    "        train_loss.append(loss_value)\n",
    "        output = 'Generation {}: Loss = {:.5f}'.format((i+1), loss_value)\n",
    "        print(output)\n",
    "        \n",
    "    if (i+1) % eval_every == 0:\n",
    "        [temp_accuracy] = sess.run([accuracy])\n",
    "        test_accuracy.append(temp_accuracy)\n",
    "        acc_output = ' --- Test Accuracy = {:.2f}%.'.format(100.*temp_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_indices = range(0, generations, eval_every)\n",
    "output_indices = range(0, generations, output_every)\n",
    "\n",
    "# 損失値をプロット\n",
    "plt.plot(output_indices, train_loss, 'k-')\n",
    "plt.title('Softmax Loss per Generation')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabe;('Softmax Loss')\n",
    "plt.show()\n",
    "\n",
    "# 正解率をプロット\n",
    "plt.plot(eval_indices, test_accuracy, 'k-')\n",
    "plt.title('Test Accuracy')\n",
    "plt.xlabel('Generation')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
