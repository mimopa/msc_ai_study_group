{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "# RNNのパラメータを設定\n",
    "min_word_freq = 5 # 出現頻度がこの値以下の単語を除外\n",
    "rnn_size = 128 # RNNモデルのサイズ（埋め込みサイズに等しい）\n",
    "epochs = 10 # データを処理する回数\n",
    "batch_size = 100 # 一度にトレーニングするサンプルの数\n",
    "learning_rate = 0.001 # 学習率（収束パラメータ）\n",
    "training_seq_len = 50 # 前後（左右）の単語の数（左右に25単語ずつ）\n",
    "embedding_size = rnn_size # 埋め込みサイズ（rnn_sizeに等しい）\n",
    "save_every = 500 # モデルを保存する頻度（500回おき）\n",
    "eval_every = 50 # テスト文を評価する頻度\n",
    "# テストのリスト\n",
    "prime_texts = ['thou art more', 'to be or not to', 'wherefore art thou']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シェイクスピアのテキストデータをダウンロードして格納\n",
    "data_dir = 'temp'\n",
    "data_file = 'shakespeare.txt'\n",
    "model_path = 'shakespeare_model'\n",
    "full_model_dir = os.path.join(data_dir, model_path)\n",
    "\n",
    "# ハイフンとアポストロフィ以外の句読点を削除\n",
    "punctuation = string.punctuation\n",
    "punctuation = ''.join([x for x in punctuation if x not in ['-', \"'\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Shakespeare Data\n",
      "Not found, downloading Shakespeare texts from www.gutenberg.org\n"
     ]
    }
   ],
   "source": [
    "# モデルフォルダを作成\n",
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)\n",
    "\n",
    "# データフォルダを作成\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    \n",
    "print('Loading Shakespeare Data')\n",
    "# ファイルがダウンロードされているかどうかを確認\n",
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    print('Not found, downloading Shakespeare texts from www.gutenberg.org')\n",
    "    shakespeare_url = 'http://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
    "    # シェイクスピアのテキストデータを取得\n",
    "    response = requests.get(shakespeare_url)\n",
    "    shakespeare_file = response.content\n",
    "    # バイナリデータを文字列にデコード\n",
    "    s_text = shakespeare_file.decode('utf-8')\n",
    "    # 最初の部分に含まれている説明用の段落を削除\n",
    "    s_text = s_text[7675:]\n",
    "    # 改行を削除\n",
    "    s_text = s_text.replace('\\r\\n', '')\n",
    "    s_text = s_text.replace('\\n', '')\n",
    "    \n",
    "    # ファイルに保存\n",
    "    with open(os.path.join(data_dir, data_file), 'w') as out_conn:\n",
    "        out_conn.write(s_text)\n",
    "else:\n",
    "    # ファイルがすでに保存されている場合は、そのファイルからデータを読み込む\n",
    "    with open(os.path.join(data_dir, data_file), 'r') as file_conn:\n",
    "        s_text = file_conn.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_text = re.sub(r'[{}]'.format(punctuation), ' ', s_text)\n",
    "s_text = re.sub('\\s+', ' ', s_text ).strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(text, min_word_freq):\n",
    "    word_counts = collections.Counter(text.split(' '))\n",
    "    # 出現頻度がしきい値を超える単語を対象とする\n",
    "    word_counts = {key:val for key, val in word_counts.items() if val>min_word_freq}\n",
    "    # 語彙からインデックスへのマッピングを作成\n",
    "    words = word_counts.keys()\n",
    "    vocab_to_ix_dict = {key:(ix+1) for ix, key in enumerate(words)}\n",
    "    # 不明なキーのインデックスとして0を追加\n",
    "    vocab_to_ix_dict['unknown']=0\n",
    "    # インデックスから語彙へのマッピングを作成\n",
    "    ix_to_vocab_dict = {val:key for key,val in vocab_to_ix_dict.items()}\n",
    "    \n",
    "    return(ix_to_vocab_dict, vocab_to_ix_dict)\n",
    "\n",
    "# シェイクスピアの語彙を作成\n",
    "ix2vocab, vocab2ix = build_vocab(s_text, min_word_freq)\n",
    "vocab_size = len(ix2vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストを単語ベクトルに変換\n",
    "s_text_words = s_text.split(' ')\n",
    "s_text_ix = []\n",
    "for ix, x in enumerate(s_text_words):\n",
    "    try:\n",
    "        s_text_ix.append(vocab2ix[x])\n",
    "    except:\n",
    "        s_text_ix.append(0)\n",
    "s_text_ix = np.array(s_text_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM RNN モデルを作成\n",
    "class LSTM_Model():\n",
    "    # モデルのすべての変数と演算を定義\n",
    "    def __init__(self, embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                 training_seq_len, vocab_size, infer_sample=False):\n",
    "        self.embedding_size = embedding_size\n",
    "        self.rnn_size = rnn_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.infer_sample = infer_sample\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if infer_sample:\n",
    "            self.batch_size = 1\n",
    "            self.training_seq_len = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            self.training_seq_len = training_seq_len\n",
    "        \n",
    "        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.rnn_size)\n",
    "        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n",
    "        \n",
    "        self.x_data = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        self.y_output = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        \n",
    "        with tf.variable_scope('lstm_vars'):\n",
    "            # ソフトマックスの出力の重み\n",
    "            W = tf.get_variable('W', [self.rnn_size, self.vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "            b = tf.get_variable('b', [self.vocab_size], tf.float32, tf.constant_initializer(0.0))\n",
    "        \n",
    "            # 埋め込みを定義\n",
    "            embedding_mat = tf.get_variable('embedding_mat', [self.vocab_size, self.embedding_size],\n",
    "                                            tf.float32, tf.random_normal_initializer())\n",
    "                                            \n",
    "            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x_data)\n",
    "            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.training_seq_len, value=embedding_output)\n",
    "            rnn_inputs_trimmed = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "        \n",
    "        # 推測中（テキストの生成中）は、ループ関数を追加\n",
    "        # i 番目の出力から i + 1 番目の入力を取得する方法を定義\n",
    "        def inferred_loop(prev, count):\n",
    "            # 隠れ層を適用\n",
    "            prev_transformed = tf.matmul(prev, W) + b\n",
    "            # 出力のインデックスを取得\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev_transformed, 1))\n",
    "            # 埋め込みベクトルを取得\n",
    "            output = tf.nn.embedding_lookup(embedding_mat, prev_symbol)\n",
    "            return(output)\n",
    "        \n",
    "        decoder = tf.contrib.legacy_seq2seq.rnn_decoder\n",
    "        outputs, last_state = decoder(rnn_inputs_trimmed,\n",
    "                                      self.initial_state,\n",
    "                                      self.lstm_cell,\n",
    "                                      loop_function=inferred_loop if infer_sample else None)\n",
    "        # 推測されていない出力\n",
    "        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, self.rnn_size])\n",
    "        # ロジットと出力\n",
    "        self.logit_output = tf.matmul(output, W) + b\n",
    "        self.model_output = tf.nn.softmax(self.logit_output)\n",
    "        \n",
    "        loss_fun = tf.contrib.legacy_seq2seq.sequence_loss_by_example\n",
    "        loss = loss_fun([self.logit_output],[tf.reshape(self.y_output, [-1])],\n",
    "                [tf.ones([self.batch_size * self.training_seq_len])],\n",
    "                self.vocab_size)\n",
    "        self.cost = tf.reduce_sum(loss) / (self.batch_size * self.training_seq_len)\n",
    "        self.final_state = last_state\n",
    "        gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), 4.5)\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
    "        \n",
    "    # サンプル（単語）をループ処理してテキストを生成\n",
    "    def sample(self, sess, words=ix2vocab, vocab=vocab2ix, num=10, prime_text='thou art'):\n",
    "        state = sess.run(self.lstm_cell.zero_state(1, tf.float32))\n",
    "        word_list = prime_text.split()\n",
    "        for word in word_list[:-1]:\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [state] = sess.run([self.final_state], feed_dict=feed_dict)\n",
    "\n",
    "        out_sentence = prime_text\n",
    "        word = word_list[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1, 1))\n",
    "            x[0, 0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [model_output, state] = sess.run([self.model_output, self.final_state], feed_dict=feed_dict)\n",
    "            sample = np.argmax(model_output[0])\n",
    "            if sample == 0:\n",
    "                break\n",
    "            word = words[sample]\n",
    "            out_sentence = out_sentence + ' ' + word\n",
    "        return(out_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM モデルを定義\n",
    "lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                        training_seq_len, vocab_size)\n",
    "\n",
    "# テストモデル定義。このスコープをテストに再利用する\n",
    "with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "    test_lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                                 training_seq_len, vocab_size, infer_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルの保存関数を定義\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "# エポックごとにバッチを作成\n",
    "num_batches = int(len(s_text_ix)/(batch_size * training_seq_len)) + 1\n",
    "# テキストインデックスを同じサイズの部分配列に分割\n",
    "batches = np.array_split(s_text_ix, num_batches)\n",
    "# それらの部分配列の形状を [batch_size, training_seq_len] に変更\n",
    "batches = [np.resize(x, [batch_size, training_seq_len]) for x in batches]\n",
    "\n",
    "# すべての変数を初期化\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Epoch #1 of 10.\n",
      "Iteration: 10, Epoch: 1, Batch: 10 out of 182, Loss: 9.77\n",
      "Iteration: 20, Epoch: 1, Batch: 20 out of 182, Loss: 9.13\n",
      "Iteration: 30, Epoch: 1, Batch: 30 out of 182, Loss: 8.56\n",
      "Iteration: 40, Epoch: 1, Batch: 40 out of 182, Loss: 8.33\n",
      "Iteration: 50, Epoch: 1, Batch: 50 out of 182, Loss: 7.88\n",
      "thou art more darts harry's octavia beastly talk'd be- through through person snow\n",
      "to be or not to be\n",
      "wherefore art thou construe boatswain devil waking bora drown'd exchange discourse he is\n",
      "Iteration: 60, Epoch: 1, Batch: 60 out of 182, Loss: 7.83\n",
      "Iteration: 70, Epoch: 1, Batch: 70 out of 182, Loss: 7.56\n",
      "Iteration: 80, Epoch: 1, Batch: 80 out of 182, Loss: 7.27\n",
      "Iteration: 90, Epoch: 1, Batch: 90 out of 182, Loss: 7.43\n",
      "Iteration: 100, Epoch: 1, Batch: 100 out of 182, Loss: 7.09\n",
      "thou art more darts only packing of the\n",
      "to be or not to be\n",
      "wherefore art thou construe conrade boatswain devil clapp'd to's talents talents alive foresee\n",
      "Iteration: 110, Epoch: 1, Batch: 110 out of 182, Loss: 7.22\n",
      "Iteration: 120, Epoch: 1, Batch: 120 out of 182, Loss: 7.11\n",
      "Iteration: 130, Epoch: 1, Batch: 130 out of 182, Loss: 6.73\n",
      "Iteration: 140, Epoch: 1, Batch: 140 out of 182, Loss: 6.86\n",
      "Iteration: 150, Epoch: 1, Batch: 150 out of 182, Loss: 6.82\n",
      "thou art more darts only packing carthage copies owner\n",
      "to be or not to the\n",
      "wherefore art thou construe conrade detest rebellion think- eve ruffian ruffian ruffian ruffian\n",
      "Iteration: 160, Epoch: 1, Batch: 160 out of 182, Loss: 6.75\n",
      "Iteration: 170, Epoch: 1, Batch: 170 out of 182, Loss: 6.85\n",
      "Iteration: 180, Epoch: 1, Batch: 180 out of 182, Loss: 6.42\n",
      "Starting Epoch #2 of 10.\n",
      "Iteration: 190, Epoch: 2, Batch: 9 out of 182, Loss: 7.22\n",
      "Iteration: 200, Epoch: 2, Batch: 19 out of 182, Loss: 7.08\n",
      "thou art more than my lord i am not i am not i\n",
      "to be or not to the\n",
      "wherefore art thou construe offend witches offend the of the\n",
      "Iteration: 210, Epoch: 2, Batch: 29 out of 182, Loss: 6.92\n",
      "Iteration: 220, Epoch: 2, Batch: 39 out of 182, Loss: 6.53\n",
      "Iteration: 230, Epoch: 2, Batch: 49 out of 182, Loss: 6.27\n",
      "Iteration: 240, Epoch: 2, Batch: 59 out of 182, Loss: 6.37\n",
      "Iteration: 250, Epoch: 2, Batch: 69 out of 182, Loss: 6.26\n",
      "thou art more than my lord lord of the\n",
      "to be or not to the\n",
      "wherefore art thou construe offend witches offend the\n",
      "Iteration: 260, Epoch: 2, Batch: 79 out of 182, Loss: 6.26\n",
      "Iteration: 270, Epoch: 2, Batch: 89 out of 182, Loss: 6.39\n",
      "Iteration: 280, Epoch: 2, Batch: 99 out of 182, Loss: 6.22\n",
      "Iteration: 290, Epoch: 2, Batch: 109 out of 182, Loss: 6.10\n",
      "Iteration: 300, Epoch: 2, Batch: 119 out of 182, Loss: 6.15\n",
      "thou art more than my lord of the\n",
      "to be or not to the\n",
      "wherefore art thou construe offend witches offend the\n",
      "Iteration: 310, Epoch: 2, Batch: 129 out of 182, Loss: 6.30\n",
      "Iteration: 320, Epoch: 2, Batch: 139 out of 182, Loss: 6.32\n",
      "Iteration: 330, Epoch: 2, Batch: 149 out of 182, Loss: 6.34\n",
      "Iteration: 340, Epoch: 2, Batch: 159 out of 182, Loss: 6.44\n",
      "Iteration: 350, Epoch: 2, Batch: 169 out of 182, Loss: 6.28\n",
      "thou art more than my lord of the\n",
      "to be or not to the\n",
      "wherefore art thou shalt shalt barber's barber's hereditary volumnius preferment gentlewomen gentlewomen gentlewomen\n",
      "Iteration: 360, Epoch: 2, Batch: 179 out of 182, Loss: 6.38\n",
      "Starting Epoch #3 of 10.\n",
      "Iteration: 370, Epoch: 3, Batch: 8 out of 182, Loss: 6.37\n",
      "Iteration: 380, Epoch: 3, Batch: 18 out of 182, Loss: 6.22\n",
      "Iteration: 390, Epoch: 3, Batch: 28 out of 182, Loss: 6.24\n",
      "Iteration: 400, Epoch: 3, Batch: 38 out of 182, Loss: 6.31\n",
      "thou art more than my lord of the\n",
      "to be or not to the\n",
      "wherefore art thou shalt shalt wrongfully barber's barber's grecians wiltshire raised driving dowry\n",
      "Iteration: 410, Epoch: 3, Batch: 48 out of 182, Loss: 6.08\n",
      "Iteration: 420, Epoch: 3, Batch: 58 out of 182, Loss: 6.33\n",
      "Iteration: 430, Epoch: 3, Batch: 68 out of 182, Loss: 6.18\n",
      "Iteration: 440, Epoch: 3, Batch: 78 out of 182, Loss: 6.24\n",
      "Iteration: 450, Epoch: 3, Batch: 88 out of 182, Loss: 6.27\n",
      "thou art more than my lord i have i am not to the\n",
      "to be or not to the\n",
      "wherefore art thou construe offend witches offend poverty villanous of illinois benedictine collegewith\n",
      "Iteration: 460, Epoch: 3, Batch: 98 out of 182, Loss: 6.20\n",
      "Iteration: 470, Epoch: 3, Batch: 108 out of 182, Loss: 6.15\n",
      "Iteration: 480, Epoch: 3, Batch: 118 out of 182, Loss: 6.20\n",
      "Iteration: 490, Epoch: 3, Batch: 128 out of 182, Loss: 6.11\n",
      "Iteration: 500, Epoch: 3, Batch: 138 out of 182, Loss: 5.84\n",
      "Model Saved To: temp/shakespeare_model/model\n",
      "thou art more than my lord of the\n",
      "to be or not to the\n",
      "wherefore art thou shalt shalt wrongfully barber's nymph trespass embassage penitent penitent solemnly\n",
      "Iteration: 510, Epoch: 3, Batch: 148 out of 182, Loss: 6.18\n",
      "Iteration: 520, Epoch: 3, Batch: 158 out of 182, Loss: 6.21\n",
      "Iteration: 530, Epoch: 3, Batch: 168 out of 182, Loss: 6.14\n",
      "Iteration: 540, Epoch: 3, Batch: 178 out of 182, Loss: 6.31\n",
      "Starting Epoch #4 of 10.\n",
      "Iteration: 550, Epoch: 4, Batch: 7 out of 182, Loss: 5.89\n",
      "thou art more than my lord of the\n",
      "to be or not to the\n",
      "wherefore art thou shalt shalt wrongfully barber's nymph trespass embassage penitent penitent solemnly\n",
      "Iteration: 560, Epoch: 4, Batch: 17 out of 182, Loss: 6.00\n",
      "Iteration: 570, Epoch: 4, Batch: 27 out of 182, Loss: 6.02\n",
      "Iteration: 580, Epoch: 4, Batch: 37 out of 182, Loss: 6.12\n",
      "Iteration: 590, Epoch: 4, Batch: 47 out of 182, Loss: 6.26\n",
      "Iteration: 600, Epoch: 4, Batch: 57 out of 182, Loss: 6.21\n",
      "thou art more than my lord of the\n",
      "to be or not to the\n",
      "wherefore art thou shalt shalt wrongfully barber's nymph trespass massacre provinces welshmen provinces\n",
      "Iteration: 610, Epoch: 4, Batch: 67 out of 182, Loss: 6.03\n",
      "Iteration: 620, Epoch: 4, Batch: 77 out of 182, Loss: 6.35\n",
      "Iteration: 630, Epoch: 4, Batch: 87 out of 182, Loss: 6.00\n",
      "Iteration: 640, Epoch: 4, Batch: 97 out of 182, Loss: 5.82\n",
      "Iteration: 650, Epoch: 4, Batch: 107 out of 182, Loss: 5.91\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou shalt shalt wrongfully barber's barber's grecians destroy'd gentlewomen gentlewomen gentlewomen\n",
      "Iteration: 660, Epoch: 4, Batch: 117 out of 182, Loss: 6.10\n",
      "Iteration: 670, Epoch: 4, Batch: 127 out of 182, Loss: 6.03\n",
      "Iteration: 680, Epoch: 4, Batch: 137 out of 182, Loss: 6.03\n",
      "Iteration: 690, Epoch: 4, Batch: 147 out of 182, Loss: 6.08\n",
      "Iteration: 700, Epoch: 4, Batch: 157 out of 182, Loss: 6.18\n",
      "thou art more than than my lord of the\n",
      "to be or not to the\n",
      "wherefore art thou shalt shalt shalt wrongfully barber's nymph trespass distinguish penitent penitent\n",
      "Iteration: 710, Epoch: 4, Batch: 167 out of 182, Loss: 6.22\n",
      "Iteration: 720, Epoch: 4, Batch: 177 out of 182, Loss: 5.92\n",
      "Starting Epoch #5 of 10.\n",
      "Iteration: 730, Epoch: 5, Batch: 6 out of 182, Loss: 6.08\n",
      "Iteration: 740, Epoch: 5, Batch: 16 out of 182, Loss: 6.10\n",
      "Iteration: 750, Epoch: 5, Batch: 26 out of 182, Loss: 6.19\n",
      "thou art more than than my lord of my lord i will not\n",
      "to be or not to the\n",
      "wherefore art thou shalt shalt shalt wrongfully nymph hanging stockings glass ghost duck\n",
      "Iteration: 760, Epoch: 5, Batch: 36 out of 182, Loss: 5.99\n",
      "Iteration: 770, Epoch: 5, Batch: 46 out of 182, Loss: 6.10\n",
      "Iteration: 780, Epoch: 5, Batch: 56 out of 182, Loss: 6.07\n",
      "Iteration: 790, Epoch: 5, Batch: 66 out of 182, Loss: 6.14\n",
      "Iteration: 800, Epoch: 5, Batch: 76 out of 182, Loss: 6.01\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou dar'st basket basket sudden helenus be not to be\n",
      "Iteration: 810, Epoch: 5, Batch: 86 out of 182, Loss: 5.93\n",
      "Iteration: 820, Epoch: 5, Batch: 96 out of 182, Loss: 5.91\n",
      "Iteration: 830, Epoch: 5, Batch: 106 out of 182, Loss: 6.15\n",
      "Iteration: 840, Epoch: 5, Batch: 116 out of 182, Loss: 6.22\n",
      "Iteration: 850, Epoch: 5, Batch: 126 out of 182, Loss: 5.94\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou dar'st basket basket entire shall be\n",
      "Iteration: 860, Epoch: 5, Batch: 136 out of 182, Loss: 5.99\n",
      "Iteration: 870, Epoch: 5, Batch: 146 out of 182, Loss: 5.96\n",
      "Iteration: 880, Epoch: 5, Batch: 156 out of 182, Loss: 5.99\n",
      "Iteration: 890, Epoch: 5, Batch: 166 out of 182, Loss: 6.22\n",
      "Iteration: 900, Epoch: 5, Batch: 176 out of 182, Loss: 6.29\n",
      "thou art more than my lord of the\n",
      "to be or not to the\n",
      "wherefore art thou dar'st basket basket sudden noble noble noble lord of the\n",
      "Starting Epoch #6 of 10.\n",
      "Iteration: 910, Epoch: 6, Batch: 5 out of 182, Loss: 5.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 920, Epoch: 6, Batch: 15 out of 182, Loss: 6.07\n",
      "Iteration: 930, Epoch: 6, Batch: 25 out of 182, Loss: 5.88\n",
      "Iteration: 940, Epoch: 6, Batch: 35 out of 182, Loss: 5.82\n",
      "Iteration: 950, Epoch: 6, Batch: 45 out of 182, Loss: 5.86\n",
      "thou art more than than the\n",
      "to be or not to the\n",
      "wherefore art thou dar'st basket basket basket dying williamshakespeare is copyright 1990-1993 by\n",
      "Iteration: 960, Epoch: 6, Batch: 55 out of 182, Loss: 5.71\n",
      "Iteration: 970, Epoch: 6, Batch: 65 out of 182, Loss: 5.97\n",
      "Iteration: 980, Epoch: 6, Batch: 75 out of 182, Loss: 5.88\n",
      "Iteration: 990, Epoch: 6, Batch: 85 out of 182, Loss: 5.91\n",
      "Iteration: 1000, Epoch: 6, Batch: 95 out of 182, Loss: 6.10\n",
      "Model Saved To: temp/shakespeare_model/model\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou dar'st basket basket basket entire shall be not a\n",
      "Iteration: 1010, Epoch: 6, Batch: 105 out of 182, Loss: 6.06\n",
      "Iteration: 1020, Epoch: 6, Batch: 115 out of 182, Loss: 5.95\n",
      "Iteration: 1030, Epoch: 6, Batch: 125 out of 182, Loss: 6.02\n",
      "Iteration: 1040, Epoch: 6, Batch: 135 out of 182, Loss: 5.90\n",
      "Iteration: 1050, Epoch: 6, Batch: 145 out of 182, Loss: 5.96\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou dar'st basket basket basket regal daughter ere ere ere ere\n",
      "Iteration: 1060, Epoch: 6, Batch: 155 out of 182, Loss: 5.84\n",
      "Iteration: 1070, Epoch: 6, Batch: 165 out of 182, Loss: 6.00\n",
      "Iteration: 1080, Epoch: 6, Batch: 175 out of 182, Loss: 5.86\n",
      "Starting Epoch #7 of 10.\n",
      "Iteration: 1090, Epoch: 7, Batch: 4 out of 182, Loss: 5.88\n",
      "Iteration: 1100, Epoch: 7, Batch: 14 out of 182, Loss: 5.77\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou dar'st basket sure shall be\n",
      "Iteration: 1110, Epoch: 7, Batch: 24 out of 182, Loss: 5.93\n",
      "Iteration: 1120, Epoch: 7, Batch: 34 out of 182, Loss: 5.91\n",
      "Iteration: 1130, Epoch: 7, Batch: 44 out of 182, Loss: 5.51\n",
      "Iteration: 1140, Epoch: 7, Batch: 54 out of 182, Loss: 5.92\n",
      "Iteration: 1150, Epoch: 7, Batch: 64 out of 182, Loss: 5.86\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou dar'st basket sure about serve my lord i am not\n",
      "Iteration: 1160, Epoch: 7, Batch: 74 out of 182, Loss: 6.21\n",
      "Iteration: 1170, Epoch: 7, Batch: 84 out of 182, Loss: 5.83\n",
      "Iteration: 1180, Epoch: 7, Batch: 94 out of 182, Loss: 5.98\n",
      "Iteration: 1190, Epoch: 7, Batch: 104 out of 182, Loss: 5.97\n",
      "Iteration: 1200, Epoch: 7, Batch: 114 out of 182, Loss: 5.78\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou dar'st basket able thy noble lord and\n",
      "Iteration: 1210, Epoch: 7, Batch: 124 out of 182, Loss: 6.03\n",
      "Iteration: 1220, Epoch: 7, Batch: 134 out of 182, Loss: 6.02\n",
      "Iteration: 1230, Epoch: 7, Batch: 144 out of 182, Loss: 5.77\n",
      "Iteration: 1240, Epoch: 7, Batch: 154 out of 182, Loss: 6.01\n",
      "Iteration: 1250, Epoch: 7, Batch: 164 out of 182, Loss: 6.02\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou dar'st basket basket dying cannot be a\n",
      "Iteration: 1260, Epoch: 7, Batch: 174 out of 182, Loss: 5.90\n",
      "Starting Epoch #8 of 10.\n",
      "Iteration: 1270, Epoch: 8, Batch: 3 out of 182, Loss: 5.79\n",
      "Iteration: 1280, Epoch: 8, Batch: 13 out of 182, Loss: 5.61\n",
      "Iteration: 1290, Epoch: 8, Batch: 23 out of 182, Loss: 5.80\n",
      "Iteration: 1300, Epoch: 8, Batch: 33 out of 182, Loss: 5.73\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou dar'st basket basket very shall be\n",
      "Iteration: 1310, Epoch: 8, Batch: 43 out of 182, Loss: 5.76\n",
      "Iteration: 1320, Epoch: 8, Batch: 53 out of 182, Loss: 5.94\n",
      "Iteration: 1330, Epoch: 8, Batch: 63 out of 182, Loss: 5.87\n",
      "Iteration: 1340, Epoch: 8, Batch: 73 out of 182, Loss: 5.78\n",
      "Iteration: 1350, Epoch: 8, Batch: 83 out of 182, Loss: 5.78\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou dar'st basket basket very shall be\n",
      "Iteration: 1360, Epoch: 8, Batch: 93 out of 182, Loss: 5.74\n",
      "Iteration: 1370, Epoch: 8, Batch: 103 out of 182, Loss: 5.79\n",
      "Iteration: 1380, Epoch: 8, Batch: 113 out of 182, Loss: 5.70\n",
      "Iteration: 1390, Epoch: 8, Batch: 123 out of 182, Loss: 5.82\n",
      "Iteration: 1400, Epoch: 8, Batch: 133 out of 182, Loss: 5.98\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou dar'st basket basket get thee a\n",
      "Iteration: 1410, Epoch: 8, Batch: 143 out of 182, Loss: 5.71\n",
      "Iteration: 1420, Epoch: 8, Batch: 153 out of 182, Loss: 5.86\n",
      "Iteration: 1430, Epoch: 8, Batch: 163 out of 182, Loss: 5.64\n",
      "Iteration: 1440, Epoch: 8, Batch: 173 out of 182, Loss: 5.91\n",
      "Starting Epoch #9 of 10.\n",
      "Iteration: 1450, Epoch: 9, Batch: 2 out of 182, Loss: 5.80\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou dar'st serve antique sword if you be\n",
      "Iteration: 1460, Epoch: 9, Batch: 12 out of 182, Loss: 5.72\n",
      "Iteration: 1470, Epoch: 9, Batch: 22 out of 182, Loss: 5.79\n",
      "Iteration: 1480, Epoch: 9, Batch: 32 out of 182, Loss: 5.86\n",
      "Iteration: 1490, Epoch: 9, Batch: 42 out of 182, Loss: 5.83\n",
      "Iteration: 1500, Epoch: 9, Batch: 52 out of 182, Loss: 5.92\n",
      "Model Saved To: temp/shakespeare_model/model\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou dar'st serve bruise serve pass a\n",
      "Iteration: 1510, Epoch: 9, Batch: 62 out of 182, Loss: 6.00\n",
      "Iteration: 1520, Epoch: 9, Batch: 72 out of 182, Loss: 5.56\n",
      "Iteration: 1530, Epoch: 9, Batch: 82 out of 182, Loss: 6.10\n",
      "Iteration: 1540, Epoch: 9, Batch: 92 out of 182, Loss: 5.70\n",
      "Iteration: 1550, Epoch: 9, Batch: 102 out of 182, Loss: 5.84\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou dar'st basket basket right for your grace is not so\n",
      "Iteration: 1560, Epoch: 9, Batch: 112 out of 182, Loss: 5.60\n",
      "Iteration: 1570, Epoch: 9, Batch: 122 out of 182, Loss: 5.81\n",
      "Iteration: 1580, Epoch: 9, Batch: 132 out of 182, Loss: 5.75\n",
      "Iteration: 1590, Epoch: 9, Batch: 142 out of 182, Loss: 5.96\n",
      "Iteration: 1600, Epoch: 9, Batch: 152 out of 182, Loss: 5.87\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou dar'st dar'st sure better than the\n",
      "Iteration: 1610, Epoch: 9, Batch: 162 out of 182, Loss: 5.63\n",
      "Iteration: 1620, Epoch: 9, Batch: 172 out of 182, Loss: 5.72\n",
      "Starting Epoch #10 of 10.\n",
      "Iteration: 1630, Epoch: 10, Batch: 1 out of 182, Loss: 5.80\n",
      "Iteration: 1640, Epoch: 10, Batch: 11 out of 182, Loss: 5.89\n",
      "Iteration: 1650, Epoch: 10, Batch: 21 out of 182, Loss: 5.73\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou dar'st dar'st sure better than the\n",
      "Iteration: 1660, Epoch: 10, Batch: 31 out of 182, Loss: 6.00\n",
      "Iteration: 1670, Epoch: 10, Batch: 41 out of 182, Loss: 5.75\n",
      "Iteration: 1680, Epoch: 10, Batch: 51 out of 182, Loss: 5.90\n",
      "Iteration: 1690, Epoch: 10, Batch: 61 out of 182, Loss: 5.78\n",
      "Iteration: 1700, Epoch: 10, Batch: 71 out of 182, Loss: 5.61\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou dar'st dar'st sure better than the\n",
      "Iteration: 1710, Epoch: 10, Batch: 81 out of 182, Loss: 5.47\n",
      "Iteration: 1720, Epoch: 10, Batch: 91 out of 182, Loss: 5.53\n",
      "Iteration: 1730, Epoch: 10, Batch: 101 out of 182, Loss: 5.71\n",
      "Iteration: 1740, Epoch: 10, Batch: 111 out of 182, Loss: 5.69\n",
      "Iteration: 1750, Epoch: 10, Batch: 121 out of 182, Loss: 5.84\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou dar'st dar'st sure better than the\n",
      "Iteration: 1760, Epoch: 10, Batch: 131 out of 182, Loss: 5.77\n",
      "Iteration: 1770, Epoch: 10, Batch: 141 out of 182, Loss: 5.89\n",
      "Iteration: 1780, Epoch: 10, Batch: 151 out of 182, Loss: 6.00\n",
      "Iteration: 1790, Epoch: 10, Batch: 161 out of 182, Loss: 5.72\n",
      "Iteration: 1800, Epoch: 10, Batch: 171 out of 182, Loss: 5.67\n",
      "thou art more than a\n",
      "to be or not to the\n",
      "wherefore art thou dar'st dar'st sure better than the\n",
      "Iteration: 1810, Epoch: 10, Batch: 181 out of 182, Loss: 5.91\n"
     ]
    }
   ],
   "source": [
    "train_loss = []\n",
    "iteration_count = 1\n",
    "for epoch in range(epochs):\n",
    "    # 単語インデックスをシャッフル\n",
    "    random.shuffle(batches)\n",
    "    # シャッフルしたバッチから目的値を作成\n",
    "    targets = [np.roll(x, -1, axis=1) for x in batches]\n",
    "    # エポックの実行を開始\n",
    "    print('Starting Epoch #{} of {}.'.format(epoch+1, epochs))\n",
    "    # LSTMの初期状態をエポックごとにリセット\n",
    "    state = sess.run(lstm_model.initial_state)\n",
    "    for ix, batch in enumerate(batches):\n",
    "        training_dict = {lstm_model.x_data: batch, lstm_model.y_output: targets[ix]}\n",
    "        c, h = lstm_model.initial_state\n",
    "        training_dict[c] = state.c\n",
    "        training_dict[h] = state.h\n",
    "        \n",
    "        temp_loss, state, _ = sess.run([lstm_model.cost, lstm_model.final_state, lstm_model.train_op],\n",
    "                                       feed_dict=training_dict)\n",
    "        train_loss.append(temp_loss)\n",
    "        \n",
    "        # 10回おきにステータスを出力\n",
    "        if iteration_count % 10 == 0:\n",
    "            summary_nums = (iteration_count, epoch+1, ix+1, num_batches+1, temp_loss)\n",
    "            print('Iteration: {}, Epoch: {}, Batch: {} out of {}, Loss: {:.2f}'.format(*summary_nums))\n",
    "        \n",
    "        # モデルと語彙を保存\n",
    "        if iteration_count % save_every == 0:\n",
    "            # モデルを保存\n",
    "            model_file_name = os.path.join(full_model_dir, 'model')\n",
    "            saver.save(sess, model_file_name, global_step = iteration_count)\n",
    "            print('Model Saved To: {}'.format(model_file_name))\n",
    "            # 語彙を保存\n",
    "            dictionary_file = os.path.join(full_model_dir, 'vocab.pkl')\n",
    "            with open(dictionary_file, 'wb') as dict_file_conn:\n",
    "                pickle.dump([vocab2ix, ix2vocab], dict_file_conn)\n",
    "        \n",
    "        if iteration_count % eval_every == 0:\n",
    "            for sample in prime_texts:\n",
    "                print(test_lstm_model.sample(sess, ix2vocab, vocab2ix, num=10, prime_text=sample))\n",
    "                \n",
    "        iteration_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc7e6174048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, 'k-')\n",
    "plt.title('Sequence to Sequence Loss')\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
