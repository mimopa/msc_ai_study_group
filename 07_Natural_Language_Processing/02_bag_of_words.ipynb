{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import string\n",
    "import requests\n",
    "import io\n",
    "from zipfile import ZipFile\n",
    "from tensorflow.contrib import learn\n",
    "from tensorflow.python.framework import ops\n",
    "ops.reset_default_graph()\n",
    "\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_file_name = os.path.join('temp','temp_spam_data.csv')\n",
    "\n",
    "# ディレクトリが存在しない場合は作成\n",
    "if not os.path.exists('temp'):\n",
    "    os.makedirs('temp')\n",
    "\n",
    "if os.path.isfile(save_file_name):\n",
    "    text_data = []\n",
    "    with open(save_file_name, 'r') as temp_output_file:\n",
    "        reader = csv.reader(temp_output_file)\n",
    "        for row in reader:\n",
    "            text_data.append(row)\n",
    "else:\n",
    "    zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'\n",
    "    r = requests.get(zip_url)\n",
    "    z = ZipFile(io.BytesIO(r.content))\n",
    "    file = z.read('SMSSpamCollection')\n",
    "    # データの書式設定\n",
    "    text_data = file.decode()\n",
    "    text_data = text_data.encode('ascii',errors='ignore')\n",
    "    text_data = text_data.decode().split('\\n')\n",
    "    text_data = [x.split('\\t') for x in text_data if len(x)>=1]\n",
    "    \n",
    "    # CSV ファイルに書き込む\n",
    "    with open(save_file_name, 'w') as temp_output_file:\n",
    "        writer = csv.writer(temp_output_file)\n",
    "        writer.writerows(text_data)\n",
    "\n",
    "texts = [x[1] for x in text_data]\n",
    "target = [x[0] for x in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'spam' のラベルを 1、'ham' のラベルを 0 に変更\n",
    "target = [1 if x=='spam' else 0 for x in target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 小文字に変換\n",
    "texts = [x.lower() for x in texts]\n",
    "\n",
    "# 句読点を削除\n",
    "texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
    "\n",
    "# 数字を削除\n",
    "texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
    "\n",
    "# 余分なホワイトスペースを削除\n",
    "texts = [' '.join(x.split()) for x in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1680b1e588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# テキストの長さをヒストグラムとしてプロット\n",
    "text_lengths = [len(x.split()) for x in texts]\n",
    "text_lengths = [x for x in text_lengths if x < 50]\n",
    "plt.hist(text_lengths, bins=25)\n",
    "plt.title('Histogram of # of Words in Texts')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_size = 25\n",
    "min_word_freq = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 語彙プロセッサを設定\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(sentence_size, min_frequency=min_word_freq)\n",
    "\n",
    "# 一意な単語の長さを取得するために適合と変換を実行\n",
    "vocab_processor.transform(texts)\n",
    "embedding_size = len([x for x in vocab_processor.transform(texts)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_indices = np.random.choice(len(texts), round(len(texts)*0.8), replace=False)\n",
    "test_indices = np.array(list(set(range(len(texts))) - set(train_indices)))\n",
    "texts_train = [x for ix, x in enumerate(texts) if ix in train_indices]\n",
    "texts_test = [x for ix, x in enumerate(texts) if ix in test_indices]\n",
    "target_train = [x for ix, x in enumerate(target) if ix in train_indices]\n",
    "target_test = [x for ix, x in enumerate(target) if ix in test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "identity_mat = tf.diag(tf.ones(shape=[embedding_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ロジスティック回帰の変数を作成\n",
    "A = tf.Variable(tf.random_normal(shape=[embedding_size,1]))\n",
    "b = tf.Variable(tf.random_normal(shape=[1,1]))\n",
    "\n",
    "# プレースホルダを初期化\n",
    "x_data = tf.placeholder(shape=[sentence_size], dtype=tf.int32)\n",
    "y_target = tf.placeholder(shape=[1, 1], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_embed = tf.nn.embedding_lookup(identity_mat, x_data)\n",
    "x_col_sums = tf.reduce_sum(x_embed, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_col_sums_2D = tf.expand_dims(x_col_sums, 0)\n",
    "model_output = tf.add(tf.matmul(x_col_sums_2D, A), b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 損失関数を設定\n",
    "loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=model_output, labels=y_target))\n",
    "# 予測演算を設定\n",
    "prediction = tf.sigmoid(model_output)\n",
    "\n",
    "# 最適化関数を設定\n",
    "my_opt = tf.train.GradientDescentOptimizer(0.001)\n",
    "train_step = my_opt.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Over 4459 Sentences.\n",
      "Training Observation #10: Loss = 1.3829\n",
      "Training Observation #20: Loss = 0.168163\n",
      "Training Observation #30: Loss = 0.624816\n",
      "Training Observation #40: Loss = 0.829035\n",
      "Training Observation #50: Loss = 1.13093\n",
      "Training Observation #60: Loss = 0.781089\n",
      "Training Observation #70: Loss = 9.40186\n",
      "Training Observation #80: Loss = 8.46548e-06\n",
      "Training Observation #90: Loss = 1.34957e-06\n",
      "Training Observation #100: Loss = 0.120943\n",
      "Training Observation #110: Loss = 0.00017632\n",
      "Training Observation #120: Loss = 0.255709\n",
      "Training Observation #130: Loss = 0.193408\n",
      "Training Observation #140: Loss = 0.000905866\n",
      "Training Observation #150: Loss = 1.5051\n",
      "Training Observation #160: Loss = 0.00519117\n",
      "Training Observation #170: Loss = 0.0113198\n",
      "Training Observation #180: Loss = 0.000295906\n",
      "Training Observation #190: Loss = 0.000339607\n",
      "Training Observation #200: Loss = 0.0185215\n",
      "Training Observation #210: Loss = 0.0108108\n",
      "Training Observation #220: Loss = 0.000468825\n",
      "Training Observation #230: Loss = 0.0121594\n",
      "Training Observation #240: Loss = 0.168324\n",
      "Training Observation #250: Loss = 0.00840791\n",
      "Training Observation #260: Loss = 0.0148383\n",
      "Training Observation #270: Loss = 0.0122834\n",
      "Training Observation #280: Loss = 0.0407599\n",
      "Training Observation #290: Loss = 0.516225\n",
      "Training Observation #300: Loss = 3.02093\n",
      "Training Observation #310: Loss = 0.368829\n",
      "Training Observation #320: Loss = 0.197239\n",
      "Training Observation #330: Loss = 0.765483\n",
      "Training Observation #340: Loss = 0.00101713\n",
      "Training Observation #350: Loss = 0.00666886\n",
      "Training Observation #360: Loss = 0.00239724\n",
      "Training Observation #370: Loss = 0.0106437\n",
      "Training Observation #380: Loss = 0.00364992\n",
      "Training Observation #390: Loss = 0.749292\n",
      "Training Observation #400: Loss = 9.16864e-05\n",
      "Training Observation #410: Loss = 0.000175594\n",
      "Training Observation #420: Loss = 0.0972871\n",
      "Training Observation #430: Loss = 0.00658204\n",
      "Training Observation #440: Loss = 0.0137311\n",
      "Training Observation #450: Loss = 0.00371251\n",
      "Training Observation #460: Loss = 0.00105679\n",
      "Training Observation #470: Loss = 0.000204786\n",
      "Training Observation #480: Loss = 0.481544\n",
      "Training Observation #490: Loss = 0.000443464\n",
      "Training Observation #500: Loss = 0.000363373\n",
      "Training Observation #510: Loss = 0.322564\n",
      "Training Observation #520: Loss = 5.45668\n",
      "Training Observation #530: Loss = 0.0574676\n",
      "Training Observation #540: Loss = 0.0112176\n",
      "Training Observation #550: Loss = 0.00157876\n",
      "Training Observation #560: Loss = 0.000453591\n",
      "Training Observation #570: Loss = 0.0755172\n",
      "Training Observation #580: Loss = 3.44484\n",
      "Training Observation #590: Loss = 3.49953\n",
      "Training Observation #600: Loss = 0.004396\n",
      "Training Observation #610: Loss = 0.00925701\n",
      "Training Observation #620: Loss = 0.140137\n",
      "Training Observation #630: Loss = 0.616713\n",
      "Training Observation #640: Loss = 0.00626379\n",
      "Training Observation #650: Loss = 0.00144725\n",
      "Training Observation #660: Loss = 0.0409803\n",
      "Training Observation #670: Loss = 11.8665\n",
      "Training Observation #680: Loss = 0.00145069\n",
      "Training Observation #690: Loss = 0.125289\n",
      "Training Observation #700: Loss = 0.0404189\n",
      "Training Observation #710: Loss = 0.00113705\n",
      "Training Observation #720: Loss = 0.001161\n",
      "Training Observation #730: Loss = 0.0492977\n",
      "Training Observation #740: Loss = 0.00887066\n",
      "Training Observation #750: Loss = 2.43256\n",
      "Training Observation #760: Loss = 7.20864\n",
      "Training Observation #770: Loss = 0.0303956\n",
      "Training Observation #780: Loss = 3.62281e-05\n",
      "Training Observation #790: Loss = 9.13401e-05\n",
      "Training Observation #800: Loss = 0.015507\n",
      "Training Observation #810: Loss = 3.59078\n",
      "Training Observation #820: Loss = 4.98133e-05\n",
      "Training Observation #830: Loss = 0.00131597\n",
      "Training Observation #840: Loss = 0.00859233\n",
      "Training Observation #850: Loss = 0.00201653\n",
      "Training Observation #860: Loss = 0.000395891\n",
      "Training Observation #870: Loss = 2.03469\n",
      "Training Observation #880: Loss = 0.000128429\n",
      "Training Observation #890: Loss = 0.148177\n",
      "Training Observation #900: Loss = 9.33137e-07\n",
      "Training Observation #910: Loss = 6.65195\n",
      "Training Observation #920: Loss = 0.0130587\n",
      "Training Observation #930: Loss = 0.0298987\n",
      "Training Observation #940: Loss = 0.0071497\n",
      "Training Observation #950: Loss = 0.000191195\n",
      "Training Observation #960: Loss = 0.000547078\n",
      "Training Observation #970: Loss = 6.05153\n",
      "Training Observation #980: Loss = 0.366055\n",
      "Training Observation #990: Loss = 1.50922\n",
      "Training Observation #1000: Loss = 0.0492374\n",
      "Training Observation #1010: Loss = 0.000445598\n",
      "Training Observation #1020: Loss = 0.000490774\n",
      "Training Observation #1030: Loss = 0.000191717\n",
      "Training Observation #1040: Loss = 5.44763\n",
      "Training Observation #1050: Loss = 0.00441707\n",
      "Training Observation #1060: Loss = 0.00672831\n",
      "Training Observation #1070: Loss = 0.00018826\n",
      "Training Observation #1080: Loss = 0.0135208\n",
      "Training Observation #1090: Loss = 6.77983\n",
      "Training Observation #1100: Loss = 0.065115\n",
      "Training Observation #1110: Loss = 4.86778\n",
      "Training Observation #1120: Loss = 1.66647\n",
      "Training Observation #1130: Loss = 0.382235\n",
      "Training Observation #1140: Loss = 0.0493234\n",
      "Training Observation #1150: Loss = 4.0525e-05\n",
      "Training Observation #1160: Loss = 0.439377\n",
      "Training Observation #1170: Loss = 7.78018\n",
      "Training Observation #1180: Loss = 0.94527\n",
      "Training Observation #1190: Loss = 0.000194601\n",
      "Training Observation #1200: Loss = 0.000751286\n",
      "Training Observation #1210: Loss = 0.255999\n",
      "Training Observation #1220: Loss = 0.00605193\n",
      "Training Observation #1230: Loss = 0.433702\n",
      "Training Observation #1240: Loss = 0.0293179\n",
      "Training Observation #1250: Loss = 0.0140924\n",
      "Training Observation #1260: Loss = 0.00143667\n",
      "Training Observation #1270: Loss = 0.0137748\n",
      "Training Observation #1280: Loss = 0.0714387\n",
      "Training Observation #1290: Loss = 3.11033e-05\n",
      "Training Observation #1300: Loss = 0.00346705\n",
      "Training Observation #1310: Loss = 0.0473302\n",
      "Training Observation #1320: Loss = 8.77623\n",
      "Training Observation #1330: Loss = 0.017672\n",
      "Training Observation #1340: Loss = 0.00100514\n",
      "Training Observation #1350: Loss = 0.0335231\n",
      "Training Observation #1360: Loss = 1.77566\n",
      "Training Observation #1370: Loss = 0.00012243\n",
      "Training Observation #1380: Loss = 0.00125435\n",
      "Training Observation #1390: Loss = 0.000440548\n",
      "Training Observation #1400: Loss = 0.000612893\n",
      "Training Observation #1410: Loss = 0.00536643\n",
      "Training Observation #1420: Loss = 0.0107438\n",
      "Training Observation #1430: Loss = 0.0272669\n",
      "Training Observation #1440: Loss = 0.00383099\n",
      "Training Observation #1450: Loss = 0.0551743\n",
      "Training Observation #1460: Loss = 0.206594\n",
      "Training Observation #1470: Loss = 1.37312\n",
      "Training Observation #1480: Loss = 0.00191742\n",
      "Training Observation #1490: Loss = 6.27582e-05\n",
      "Training Observation #1500: Loss = 0.000229655\n",
      "Training Observation #1510: Loss = 0.0461863\n",
      "Training Observation #1520: Loss = 2.83259e-06\n",
      "Training Observation #1530: Loss = 0.0717172\n",
      "Training Observation #1540: Loss = 0.367536\n",
      "Training Observation #1550: Loss = 8.45645e-05\n",
      "Training Observation #1560: Loss = 3.26895\n",
      "Training Observation #1570: Loss = 0.186579\n",
      "Training Observation #1580: Loss = 1.0272\n",
      "Training Observation #1590: Loss = 0.00498506\n",
      "Training Observation #1600: Loss = 0.000879813\n",
      "Training Observation #1610: Loss = 0.00389123\n",
      "Training Observation #1620: Loss = 4.71499e-05\n",
      "Training Observation #1630: Loss = 3.39665\n",
      "Training Observation #1640: Loss = 0.000740764\n",
      "Training Observation #1650: Loss = 0.00155545\n",
      "Training Observation #1660: Loss = 0.00045946\n",
      "Training Observation #1670: Loss = 0.00486947\n",
      "Training Observation #1680: Loss = 0.00919986\n",
      "Training Observation #1690: Loss = 3.49198\n",
      "Training Observation #1700: Loss = 0.0476948\n",
      "Training Observation #1710: Loss = 7.05869e-05\n",
      "Training Observation #1720: Loss = 0.000321229\n",
      "Training Observation #1730: Loss = 8.4995e-05\n",
      "Training Observation #1740: Loss = 2.82589e-05\n",
      "Training Observation #1750: Loss = 0.0123479\n",
      "Training Observation #1760: Loss = 1.33439e-05\n",
      "Training Observation #1770: Loss = 2.73454e-05\n",
      "Training Observation #1780: Loss = 3.24608e-05\n",
      "Training Observation #1790: Loss = 8.5188e-05\n",
      "Training Observation #1800: Loss = 0.113004\n",
      "Training Observation #1810: Loss = 0.00253133\n",
      "Training Observation #1820: Loss = 0.0799408\n",
      "Training Observation #1830: Loss = 0.600417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Observation #1840: Loss = 9.3379e-07\n",
      "Training Observation #1850: Loss = 0.00782798\n",
      "Training Observation #1860: Loss = 0.000874001\n",
      "Training Observation #1870: Loss = 0.00800552\n",
      "Training Observation #1880: Loss = 0.00098489\n",
      "Training Observation #1890: Loss = 0.00131933\n",
      "Training Observation #1900: Loss = 0.00286281\n",
      "Training Observation #1910: Loss = 0.000599605\n",
      "Training Observation #1920: Loss = 0.0451879\n",
      "Training Observation #1930: Loss = 0.0035562\n",
      "Training Observation #1940: Loss = 0.181744\n",
      "Training Observation #1950: Loss = 0.000183034\n",
      "Training Observation #1960: Loss = 0.00378082\n",
      "Training Observation #1970: Loss = 0.000252055\n",
      "Training Observation #1980: Loss = 4.12983e-05\n",
      "Training Observation #1990: Loss = 0.723565\n",
      "Training Observation #2000: Loss = 2.5297\n",
      "Training Observation #2010: Loss = 0.000256115\n",
      "Training Observation #2020: Loss = 0.0744991\n",
      "Training Observation #2030: Loss = 0.00204064\n",
      "Training Observation #2040: Loss = 0.00355064\n",
      "Training Observation #2050: Loss = 0.0769273\n",
      "Training Observation #2060: Loss = 9.63775e-07\n",
      "Training Observation #2070: Loss = 5.39775e-05\n",
      "Training Observation #2080: Loss = 0.0197177\n",
      "Training Observation #2090: Loss = 3.42343\n",
      "Training Observation #2100: Loss = 0.153161\n",
      "Training Observation #2110: Loss = 0.0796476\n",
      "Training Observation #2120: Loss = 0.000587112\n",
      "Training Observation #2130: Loss = 0.000288463\n",
      "Training Observation #2140: Loss = 0.0194449\n",
      "Training Observation #2150: Loss = 0.00425674\n",
      "Training Observation #2160: Loss = 1.70944\n",
      "Training Observation #2170: Loss = 0.00697815\n",
      "Training Observation #2180: Loss = 0.00785303\n",
      "Training Observation #2190: Loss = 0.00010829\n",
      "Training Observation #2200: Loss = 0.000767008\n",
      "Training Observation #2210: Loss = 3.73298\n",
      "Training Observation #2220: Loss = 6.72563\n",
      "Training Observation #2230: Loss = 0.00847954\n",
      "Training Observation #2240: Loss = 1.97972\n",
      "Training Observation #2250: Loss = 4.48745e-05\n",
      "Training Observation #2260: Loss = 0.217966\n",
      "Training Observation #2270: Loss = 0.00137705\n",
      "Training Observation #2280: Loss = 6.17596e-05\n",
      "Training Observation #2290: Loss = 0.00111762\n",
      "Training Observation #2300: Loss = 0.000700792\n",
      "Training Observation #2310: Loss = 0.0128692\n",
      "Training Observation #2320: Loss = 4.23439e-05\n",
      "Training Observation #2330: Loss = 0.000354367\n",
      "Training Observation #2340: Loss = 8.75549e-05\n",
      "Training Observation #2350: Loss = 0.00360184\n",
      "Training Observation #2360: Loss = 7.32688e-06\n",
      "Training Observation #2370: Loss = 0.117435\n",
      "Training Observation #2380: Loss = 9.29851e-05\n",
      "Training Observation #2390: Loss = 0.0920257\n",
      "Training Observation #2400: Loss = 0.011249\n",
      "Training Observation #2410: Loss = 7.03313e-05\n",
      "Training Observation #2420: Loss = 0.0132544\n",
      "Training Observation #2430: Loss = 0.000154511\n",
      "Training Observation #2440: Loss = 0.00843013\n",
      "Training Observation #2450: Loss = 0.0129969\n",
      "Training Observation #2460: Loss = 0.298021\n",
      "Training Observation #2470: Loss = 0.000335611\n",
      "Training Observation #2480: Loss = 0.13839\n",
      "Training Observation #2490: Loss = 0.00222333\n",
      "Training Observation #2500: Loss = 0.115771\n",
      "Training Observation #2510: Loss = 0.000421643\n",
      "Training Observation #2520: Loss = 3.36634\n",
      "Training Observation #2530: Loss = 0.000534933\n",
      "Training Observation #2540: Loss = 0.0247573\n",
      "Training Observation #2550: Loss = 2.62537e-05\n",
      "Training Observation #2560: Loss = 8.30474e-06\n",
      "Training Observation #2570: Loss = 0.0453798\n",
      "Training Observation #2580: Loss = 0.000279111\n",
      "Training Observation #2590: Loss = 0.000578838\n",
      "Training Observation #2600: Loss = 0.00390875\n",
      "Training Observation #2610: Loss = 1.88919\n",
      "Training Observation #2620: Loss = 0.00160708\n",
      "Training Observation #2630: Loss = 0.270675\n",
      "Training Observation #2640: Loss = 0.00428646\n",
      "Training Observation #2650: Loss = 1.06399\n",
      "Training Observation #2660: Loss = 5.91992e-05\n",
      "Training Observation #2670: Loss = 0.0142861\n",
      "Training Observation #2680: Loss = 1.60123\n",
      "Training Observation #2690: Loss = 0.00164208\n",
      "Training Observation #2700: Loss = 0.23503\n",
      "Training Observation #2710: Loss = 4.0448\n",
      "Training Observation #2720: Loss = 4.29083\n",
      "Training Observation #2730: Loss = 0.0274424\n",
      "Training Observation #2740: Loss = 2.23958\n",
      "Training Observation #2750: Loss = 0.29935\n",
      "Training Observation #2760: Loss = 0.0101536\n",
      "Training Observation #2770: Loss = 0.217832\n",
      "Training Observation #2780: Loss = 0.000532703\n",
      "Training Observation #2790: Loss = 0.486535\n",
      "Training Observation #2800: Loss = 0.000101387\n",
      "Training Observation #2810: Loss = 4.78554\n",
      "Training Observation #2820: Loss = 3.70383e-05\n",
      "Training Observation #2830: Loss = 0.00214394\n",
      "Training Observation #2840: Loss = 0.000611362\n",
      "Training Observation #2850: Loss = 1.23662\n",
      "Training Observation #2860: Loss = 0.247454\n",
      "Training Observation #2870: Loss = 2.80771\n",
      "Training Observation #2880: Loss = 0.00571036\n",
      "Training Observation #2890: Loss = 0.000205643\n",
      "Training Observation #2900: Loss = 9.8885e-07\n",
      "Training Observation #2910: Loss = 0.0251149\n",
      "Training Observation #2920: Loss = 4.35694\n",
      "Training Observation #2930: Loss = 0.0089352\n",
      "Training Observation #2940: Loss = 0.0116946\n",
      "Training Observation #2950: Loss = 0.00108427\n",
      "Training Observation #2960: Loss = 0.00025137\n",
      "Training Observation #2970: Loss = 7.2912e-06\n",
      "Training Observation #2980: Loss = 1.86587e-05\n",
      "Training Observation #2990: Loss = 0.0267224\n",
      "Training Observation #3000: Loss = 0.00498698\n",
      "Training Observation #3010: Loss = 9.94116\n",
      "Training Observation #3020: Loss = 0.360409\n",
      "Training Observation #3030: Loss = 0.00036182\n",
      "Training Observation #3040: Loss = 0.730107\n",
      "Training Observation #3050: Loss = 0.00129179\n",
      "Training Observation #3060: Loss = 0.00253611\n",
      "Training Observation #3070: Loss = 0.467166\n",
      "Training Observation #3080: Loss = 0.0211299\n",
      "Training Observation #3090: Loss = 7.15843\n",
      "Training Observation #3100: Loss = 0.0702522\n",
      "Training Observation #3110: Loss = 0.00298102\n",
      "Training Observation #3120: Loss = 0.000206949\n",
      "Training Observation #3130: Loss = 0.0014544\n",
      "Training Observation #3140: Loss = 7.45898e-05\n",
      "Training Observation #3150: Loss = 1.91577\n",
      "Training Observation #3160: Loss = 0.0196333\n",
      "Training Observation #3170: Loss = 0.00697356\n",
      "Training Observation #3180: Loss = 0.02176\n",
      "Training Observation #3190: Loss = 0.00989172\n",
      "Training Observation #3200: Loss = 1.40076\n",
      "Training Observation #3210: Loss = 0.00509524\n",
      "Training Observation #3220: Loss = 0.234347\n",
      "Training Observation #3230: Loss = 4.39794e-05\n",
      "Training Observation #3240: Loss = 0.000596716\n",
      "Training Observation #3250: Loss = 0.000885444\n",
      "Training Observation #3260: Loss = 0.00206747\n",
      "Training Observation #3270: Loss = 0.00583824\n",
      "Training Observation #3280: Loss = 0.189076\n",
      "Training Observation #3290: Loss = 0.000107336\n",
      "Training Observation #3300: Loss = 1.51637\n",
      "Training Observation #3310: Loss = 0.0262675\n",
      "Training Observation #3320: Loss = 7.43771e-05\n",
      "Training Observation #3330: Loss = 4.75342e-05\n",
      "Training Observation #3340: Loss = 1.81498\n",
      "Training Observation #3350: Loss = 0.00194524\n",
      "Training Observation #3360: Loss = 10.7814\n",
      "Training Observation #3370: Loss = 2.15537\n",
      "Training Observation #3380: Loss = 0.00443109\n",
      "Training Observation #3390: Loss = 0.0478924\n",
      "Training Observation #3400: Loss = 0.120171\n",
      "Training Observation #3410: Loss = 0.00113425\n",
      "Training Observation #3420: Loss = 0.0314938\n",
      "Training Observation #3430: Loss = 1.39331e-06\n",
      "Training Observation #3440: Loss = 0.644538\n",
      "Training Observation #3450: Loss = 0.0258079\n",
      "Training Observation #3460: Loss = 7.32971e-05\n",
      "Training Observation #3470: Loss = 0.0600463\n",
      "Training Observation #3480: Loss = 0.00279068\n",
      "Training Observation #3490: Loss = 0.000765153\n",
      "Training Observation #3500: Loss = 0.000191679\n",
      "Training Observation #3510: Loss = 0.000842946\n",
      "Training Observation #3520: Loss = 0.000450351\n",
      "Training Observation #3530: Loss = 0.303473\n",
      "Training Observation #3540: Loss = 8.66158\n",
      "Training Observation #3550: Loss = 0.000226754\n",
      "Training Observation #3560: Loss = 0.000480743\n",
      "Training Observation #3570: Loss = 0.000159705\n",
      "Training Observation #3580: Loss = 0.00368973\n",
      "Training Observation #3590: Loss = 7.68309e-06\n",
      "Training Observation #3600: Loss = 0.000663952\n",
      "Training Observation #3610: Loss = 0.00032442\n",
      "Training Observation #3620: Loss = 5.67606e-05\n",
      "Training Observation #3630: Loss = 0.000160459\n",
      "Training Observation #3640: Loss = 6.97704e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Observation #3650: Loss = 3.87848\n",
      "Training Observation #3660: Loss = 0.0135814\n",
      "Training Observation #3670: Loss = 1.16938\n",
      "Training Observation #3680: Loss = 0.00162746\n",
      "Training Observation #3690: Loss = 0.00217232\n",
      "Training Observation #3700: Loss = 0.000690775\n",
      "Training Observation #3710: Loss = 0.000483281\n",
      "Training Observation #3720: Loss = 0.000240928\n",
      "Training Observation #3730: Loss = 4.54328e-05\n",
      "Training Observation #3740: Loss = 0.0792721\n",
      "Training Observation #3750: Loss = 0.00726519\n",
      "Training Observation #3760: Loss = 0.00477697\n",
      "Training Observation #3770: Loss = 1.17415\n",
      "Training Observation #3780: Loss = 0.000130687\n",
      "Training Observation #3790: Loss = 0.000415196\n",
      "Training Observation #3800: Loss = 2.36095\n",
      "Training Observation #3810: Loss = 0.00396371\n",
      "Training Observation #3820: Loss = 0.000152011\n",
      "Training Observation #3830: Loss = 0.0407253\n",
      "Training Observation #3840: Loss = 0.0300739\n",
      "Training Observation #3850: Loss = 0.0156163\n",
      "Training Observation #3860: Loss = 6.8248\n",
      "Training Observation #3870: Loss = 0.00333978\n",
      "Training Observation #3880: Loss = 0.599393\n",
      "Training Observation #3890: Loss = 0.180334\n",
      "Training Observation #3900: Loss = 0.00140343\n",
      "Training Observation #3910: Loss = 2.62749\n",
      "Training Observation #3920: Loss = 0.00015836\n",
      "Training Observation #3930: Loss = 1.83957\n",
      "Training Observation #3940: Loss = 1.05601e-05\n",
      "Training Observation #3950: Loss = 0.00035549\n",
      "Training Observation #3960: Loss = 0.00284049\n",
      "Training Observation #3970: Loss = 3.96685\n",
      "Training Observation #3980: Loss = 0.000234576\n",
      "Training Observation #3990: Loss = 0.0149673\n",
      "Training Observation #4000: Loss = 7.48601e-05\n",
      "Training Observation #4010: Loss = 1.48005\n",
      "Training Observation #4020: Loss = 0.0252436\n",
      "Training Observation #4030: Loss = 0.00298225\n",
      "Training Observation #4040: Loss = 0.0021608\n",
      "Training Observation #4050: Loss = 0.0129883\n",
      "Training Observation #4060: Loss = 2.9526e-05\n",
      "Training Observation #4070: Loss = 1.06245e-06\n",
      "Training Observation #4080: Loss = 0.00157799\n",
      "Training Observation #4090: Loss = 2.60194\n",
      "Training Observation #4100: Loss = 0.0686197\n",
      "Training Observation #4110: Loss = 0.0539858\n",
      "Training Observation #4120: Loss = 0.00197597\n",
      "Training Observation #4130: Loss = 0.00219901\n",
      "Training Observation #4140: Loss = 0.000841126\n",
      "Training Observation #4150: Loss = 0.000141235\n",
      "Training Observation #4160: Loss = 0.00713263\n",
      "Training Observation #4170: Loss = 4.13806e-05\n",
      "Training Observation #4180: Loss = 0.00141764\n",
      "Training Observation #4190: Loss = 9.71777e-05\n",
      "Training Observation #4200: Loss = 0.00051013\n",
      "Training Observation #4210: Loss = 0.00036593\n",
      "Training Observation #4220: Loss = 0.000153782\n",
      "Training Observation #4230: Loss = 0.233734\n",
      "Training Observation #4240: Loss = 0.00688989\n",
      "Training Observation #4250: Loss = 0.0735847\n",
      "Training Observation #4260: Loss = 0.00158725\n",
      "Training Observation #4270: Loss = 2.88799e-06\n",
      "Training Observation #4280: Loss = 0.00449501\n",
      "Training Observation #4290: Loss = 0.00705259\n",
      "Training Observation #4300: Loss = 0.787101\n",
      "Training Observation #4310: Loss = 7.41646e-06\n",
      "Training Observation #4320: Loss = 0.000134099\n",
      "Training Observation #4330: Loss = 0.00027477\n",
      "Training Observation #4340: Loss = 0.01859\n",
      "Training Observation #4350: Loss = 0.00231025\n",
      "Training Observation #4360: Loss = 2.26667\n",
      "Training Observation #4370: Loss = 1.60788\n",
      "Training Observation #4380: Loss = 0.000102613\n",
      "Training Observation #4390: Loss = 0.000127484\n",
      "Training Observation #4400: Loss = 0.00468544\n",
      "Training Observation #4410: Loss = 0.00417578\n",
      "Training Observation #4420: Loss = 0.000679793\n",
      "Training Observation #4430: Loss = 6.64322\n",
      "Training Observation #4440: Loss = 2.4738e-05\n",
      "Training Observation #4450: Loss = 2.98567e-06\n"
     ]
    }
   ],
   "source": [
    "# ロジスティック回帰の開始\n",
    "print('Starting Training Over {} Sentences.'.format(len(texts_train)))\n",
    "loss_vec = []\n",
    "train_acc_all = []\n",
    "train_acc_avg = []\n",
    "for ix, t in enumerate(vocab_processor.fit_transform(texts_train)):\n",
    "    y_data = [[target_train[ix]]]\n",
    "    \n",
    "    \n",
    "    sess.run(train_step, feed_dict={x_data: t, y_target: y_data})\n",
    "    temp_loss = sess.run(loss, feed_dict={x_data: t, y_target: y_data})\n",
    "    loss_vec.append(temp_loss)\n",
    "    \n",
    "    if (ix+1)%10==0:\n",
    "        print('Training Observation #' + str(ix+1) + ': Loss = ' + str(temp_loss))\n",
    "        \n",
    "    # 最後の50 回の観測について正解率の移動平均を追跡\n",
    "    # 1つの観測の予測値を取得\n",
    "    [[temp_pred]] = sess.run(prediction, feed_dict={x_data:t, y_target:y_data})\n",
    "    # 予測が正解かどうかを真偽値で取得\n",
    "    train_acc_temp = target_train[ix]==np.round(temp_pred)\n",
    "    train_acc_all.append(train_acc_temp)\n",
    "    if len(train_acc_all) >= 50:\n",
    "        train_acc_avg.append(np.mean(train_acc_all[-50:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
